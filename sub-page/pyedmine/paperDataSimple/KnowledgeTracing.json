{
    "task": "KnowledgeTracing",
    "papers": [
        {
            "model_name": "",
            "paper_title": "Incorporating Rich Features into Deep Knowledge Tracing",
            "abstract": "The desire to follow student learning within intelligent tutoring systems in near real time has led to the development of several models anticipating the correctness of the next item as students work through an assignment. Such models have included Bayesian Knowledge Tracing (BKT), Performance Factors Analysis (PFA), and more recently with developments in deep learning, Deep Knowledge Tracing (DKT). This DKT model, based on the use of a recurrent neural network, exhibited promising results. Thus far, however, the model has only considered the knowledge components of the problems and correctness as input, neglecting the breadth of other features collected by computer-based learning platforms. This work seeks to improve upon the DKT model by incorporating more features at the problem-level. With this higher dimensional input, an adaption to the original DKT model structure is also proposed, incorporating an auto-encoder network layer to convert the input into a low dimensional feature vector to reduce both the resource requirement and time needed to train. Experiment results show that our adapted DKT model, observing more combinations of features, can effectively improve accuracy.",
            "source": "L@S[C]",
            "year": "2017",
            "paper_file_name": "2017-L@S-Incorporating Rich Features into Deep Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Predictive Student Modeling in Educational Games with Multi-Task Learning",
            "abstract": "Modeling student knowledge is critical in adaptive learning environments. Predictive student modeling enables formative assessment of student knowledge and skills, and it drives personalized support to create learning experiences that are both effective and engaging. Traditional approaches to predictive student modeling utilize features extracted from performance, aggregating student test performance as a single output label. We reformulate predictive student modeling as a multi-task learning problem, modeling questions from student of this approach by utilizing student data from a series of laboratory-based and classroom-based studies conducted with a game-based learning environment for microbiology education, CRYSTAL ISLAND. Using sequential representations of student gameplay, results show that multi-task stacked LSTMs with residual connections significantly outperform baseline models that do not use the multi-task formulation. Additionally, the accuracy of predictive student models is improved as the number of tasks increases. These findings have significant implications for the design and development of predictive student models in adaptive learning environments.",
            "source": "AAAI[C]",
            "year": "2020",
            "paper_file_name": "KnowledgeTracing    2020-AAAI-Predictive Student Modeling in Educational Games with Multi-Task Learning.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge",
            "abstract": "Digital technologies are becoming increasingly prevalent in education, enabling personalized, high quality education resources to be accessible by students across the world. Importantly, among these resources are diagnostic questions: the answers that the students give to these questions reveal key information about the specific nature of misconceptions that the students may hold. Analyzing the massive quantities of data stemming from students' interactions with these diagnostic questions can help us more accurately understand the students' learning status and thus allow us to automate learning curriculum recommendations. In this competition, participants will focus on the students' answer records to these multiple-choice diagnostic questions, with the aim of 1) accurately predicting which answers the students provide; 2) accurately predicting which questions have high quality; and 3) determining a personalized sequence of questions for each student that best predicts the student's answers. These tasks closely mimic the goals of a real-world educational platform and are highly representative of the educational challenges faced today. We provide over 20 million examples of students' answers to mathematics questions from Eedi, a leading educational platform which thousands of students interact with daily around the globe. Participants to this competition have a chance to make a lasting, real-world impact on the quality of personalized education for millions of students across the world.",
            "source": "NeurIPS[C]",
            "year": "2020",
            "paper_file_name": "2020-NeurIPS-Competition-Education Challenge.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Tracking student progress in a game-like learning environment with a Monte Carlo Bayesian Knowledge Tracing model",
            "abstract": "The Bayesian Knowledge Tracing (BKT) model is a popular model used for tracking student progress in learning systems such as an intelligent tutoring system. However, the model is not free of problems. Well-recognized problems include the identiﬁability problem and the empirical degeneracy problem. Unfortunately, these problems are still poorly understood and how they should be dealt with in practice is unclear. Here, we analyze the mathematical structure of the BKT model, identify a source of the diﬃculty, and construct a simple Monte Carlo BKT model to analyze the problem in real data. Using the student activity data obtained from the ramp task module at the Concord Consortium, we ﬁnd that the Monte Carlo BKT analysis is capable of detecting the identiﬁability problem and the empirical degeneracy problem, and, more generally, gives an excellent summary of the student learning data. In particular, the student activity monitoring parameter M emerges as the central parameter.",
            "source": "LAK[C]",
            "year": "2015",
            "paper_file_name": "2015-LAK-Tracking student progress in a game-like learning environment with a Monte Carlo Bayesian Knowledge Tracing model.pdf"
        },
        {
            "model_name": "GIKT",
            "paper_title": "GIKT: A Graph-Based Interaction Model for Knowledge Tracing",
            "abstract": "Abstract. With the rapid development in online education, knowledge tracing (KT) has become a fundamental problem which traces students' knowledge status and predicts their performance on new questions. Questions are often numerous in online education systems, and are always associated with much fewer skills. However, the previous literature fails to involve question information together with high-order question-skill correlations, which is mostly limited by data sparsity and multi-skill problems. From the model perspective, previous models can hardly capture the long-term dependency of student exercise history, and cannot model the interactions between student-questions, and student-skills in a consistent way. In this paper, we propose a Graph-based Interaction model for Knowledge Tracing (GIKT) to tackle the above problems. More specifically, GIKT utilizes graph convolutional network (GCN) to substantially incorporate question-skill correlations via embedding propagation. Besides, considering that relevant questions are usually scattered throughout the exercise history, and that question and skill are just different instantiations of knowledge, GIKT generalizes the degree of students' master of the question to the interactions between the student's current state, the student's history states, the target question, and related skills. Experiments on three datasets demonstrate that GIKT achieves the new state-of-the-art performance, with 2%–6% absolute AUC improvement.",
            "source": "PKDD[C]",
            "year": "2020",
            "paper_file_name": "2020-PKDD-GIKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Architectural Adaptation and Regularization of Attention Networks for Incremental Knowledge Tracing",
            "abstract": "EdTech platforms continuously refresh their database with new\nquestions and concepts with evolving course syllabus. The state-\nof-the-art knowledge tracing models are unable to adapt to these\nchanges, as the size of the question embedding layers is typically\nfixed. In this work, we propose an incremental learning algorithm\nfor knowledge tracing that is capable of adapting itself to growing\npool of concepts and questions, through its architectural adaptation\nand regularization strategies. The algorithm, referred as, \"Archi-\ntectural adaptation and Regularization of Attention network for\nIncremental Knowledge Tracing (ARAIKT)\", is capable of adapting\nthe embeddings with increasing concepts and question bank, while\npreserving representations of the previous concepts and question\nbanks. Furthermore, they are robust to distributional drifts in the\ndata, and are capable of preserving privacy of data across study\ncenters and EdTech platforms. We demonstrate the effectiveness of\nthe ARAIKT by evaluating its performance on subsets of study cen-\nters/academic years within ASSISTment2009 and ASSISTment2017\ndata sets, respectively.",
            "source": "LAK[C]",
            "year": "2024",
            "paper_file_name": "2024-LAK-Architectural Adaptation and Regularization of Attention Networks for Incremental Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Investigating Algorithmic Bias on Bayesian Knowledge Tracing and Carelessness Detectors",
            "abstract": "In today's data-driven educational technologies, algorithms have a pivotal impact on student experiences and outcomes. Therefore, it is critical to take steps to minimize biases, to avoid perpetuating or exacerbating inequalities. In this paper, we investigate the degree to which algorithmic biases are present in two learning analytics models: knowledge estimates based on Bayesian Knowledge Tracing (BKT) and carelessness detectors. Using data from a learning platform used across the United States at scale, we explore algorithmic bias following three different approaches: 1) analyzing the performance of the models on every demographic group in the sample, 2) comparing performance across intersectional groups of these demographics, and 3) investigating whether the models trained using specific groups can be transferred to demographics that were not observed during the training process. Our experimental results show that the performance of these models is close to equal across all the demographic and intersectional groups. These findings establish the feasibility of validating educational algorithms for intersectional groups and indicate that these algorithms can be fairly used for diverse students at scale.",
            "source": "LAK[C]",
            "year": "2024",
            "paper_file_name": "2024-LAK-Investigating Algorithmic Bias on Bayesian Knowledge Tracing and Carelessness Detectors.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Knowledge Tracing as Language Processing: A Large-Scale Autoregressive Paradigm",
            "abstract": "Abstract. Knowledge tracing (KT) is the process of modelling students’ cognitive states to forecast their future academic performance, using their historical learning interactions as a reference. Recent scholarly investigations have introduced a range of deep learning-based knowledge tracing (DLKT) methodologies, which have demonstrated considerable potential in their outcomes. Considering the excellent performance of large models in various domains, we have explored the possibility of migrating their architecture to the KT domain. We posit that the eﬃcacy of the large language model (LLM) can be largely attributed to the utilization of an auto-regressive Transformer decoder, which facilitates the learning of comprehensive representations and the processing of extensive data. Hence, we propose a DLKT model, LLM-KT, which is based on the LLM architecture. This model addresses the long-term dependency between students’ historical interactions and their subsequent performance through a stack of Transformer decoders. To fully utilize the potential of large models, we evaluated our model capabilities on EdNet, which is currently the world’s largest real KT dataset. Through a series of quantitative and qualitative experimental analyses, we answer two key questions: (1) is it feasible to apply the LLM-like architectures in the KT domain? (2) can the continuous extension of models improve prediction performance in KT? To encourage reproducible research, we make our data and code publicly available at https://github.com/ai4ed/AIED2024-LLM-KT.",
            "source": "AIED[C]",
            "year": "2024",
            "paper_file_name": "2024-AIED-Knowledge Tracing as Language Processing- A Large-Scale Autoregressive Paradigm.pdf"
        },
        {
            "model_name": "HGKT",
            "paper_title": "Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) which aims at predicting learner's knowledge mastery plays an important role in the computer-aided educational system. The goal of KT is to provide personalized learning paths for learners by diagnosing the mastery of each knowledge, thus improving the learning efficiency. In recent years, many deep learning models have been applied to tackle the KT task, which has shown promising results. However, most existing methods simplify the exercising records as knowledge sequences, which fail to explore the rich information that existed in exercises. Besides, the existing diagnosis results of knowledge tracing are not convincing enough since they neglect hierarchical relations between exercises. To solve the above problems, we propose a hierarchical graph knowledge tracing model called HGKT to explore the latent complex relations between exercises. Specifically, we introduce the concept of problem schema to construct a hierarchical exercise graph that could model the exercise learning dependencies. Moreover, we employ two attention mechanisms to highlight important historical states of learners. In the testing stage, we present a knowledge&schema diagnosis matrix that could trace the transition of mastery of knowledge and problem schema, which can be more easily applied to different applications. Extensive experiments show the effectiveness and interpretability of our proposed model.",
            "source": "SIGIR[C]",
            "year": "2022",
            "paper_file_name": "2022-SIGIR-HGKT@@.pdf"
        },
        {
            "model_name": "HD-KT",
            "paper_title": "HD-KT: Advancing Robust Knowledge Tracing via Anomalous Learning Interaction Detection",
            "abstract": "Knowledge tracing (KT) is a crucial task in online learning, aimed at tracing and predicting each student's knowledge states throughout their learning process. Over the past decade, it has garnered wide-spread attention due to it provides the potential for more tailored and adaptive online learning experiences. Although most current KT methodologies emphasize optimizing network structures to enhance predictive accuracy for future student performance, they often neglect anomalous interactions in students' learning processes, which may arise from low data quality (i.e., inferior question quality) and abnormal student behaviors (i.e., guessing and mistakes). To this end, in this paper, we propose a novel framework, termed HD-KT, designed to enhance the robustness of existing KT methodologies with Hybrid learning interactions Denoising approach. Specifically, we introduce two detectors for anomalous learning interactions, namely knowledge state-guided anomaly detector and student profile-guided anomaly detector. In the first detection module, we design a sequential autoencoder to identify anomalous learning interactions by detecting atypical student knowledge states. In the second module, we incorporate an attention mechanism by modeling a student's long-term profile to capture irregular interactions. Extensive experiments on four real-world benchmark datasets have decisively shown our HD-KT markedly boosts the robustness of numerous prevailing KT models, consequently increasing the accuracy of future student performance predictions. Additionally, our case studies highlight the versatility of HD-KT in addressing diverse downstream tasks, such as exercise quality analysis and learning behavior-based student clustering.",
            "source": "WWW[C]",
            "year": "2024",
            "paper_file_name": "2024-WWW-HD-KT@@Advancing Robust Knowledge Tracing via Anomalous Learning Interaction Detection.pdf"
        },
        {
            "model_name": "LFBKT",
            "paper_title": "Knowledge Tracing Model with Learning and Forgetting Behavior",
            "abstract": "The Knowledge Tracing (KT) task aims to trace the changes of students' knowledge state in real time according to students' historical learning behavior, and predict students' future learning performance. The modern KT models have two problems. One is that these KT models can't reflect students' actual knowledge level. Most KT models only judge students' knowledge state based on their performance in exercises, and poor performance will lead to a decline in knowledge state. However, the essence of students' learning process is the process of acquiring knowledge, which is also a manifestation of learning behavior. Even if they answer the exercises incorrectly, they will still gain knowledge. The other problem is that many KT models don't pay enough attention to the impact of students' forgetting behavior on the knowledge state in the learning process. In fact, learning and forgetting behavior run through students' learning process, and their effects on students' knowledge state shouldn't be ignored. In this paper, based on educational psychology theory, we propose a knowledge tracing model with learning and forgetting behavior (LFBKT). LFBKT comprehensively considers the factors that affect learning and forgetting behavior to build the knowledge acquisition layer, knowledge absorption layer and knowledge forgetting layer. In addition, LFBKT introduces difficulty information to enrich the information of the exercise itself, while taking into account other answering performances besides the answer. Experimental results on two public datasets show that LFBKT can better trace students' knowledge state and outperforms existing models in terms of ACC and AUC.",
            "source": "CIKM[C]",
            "year": "2022",
            "paper_file_name": "2022-CIKM-LFBKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Knowledge Tracing Machines: Factorization Machines for Knowledge Tracing",
            "abstract": "Knowledge tracing is a sequence prediction problem where\nthe goal is to predict the outcomes of students over questions\nas they are interacting with a learning platform. By tracking\nthe evolution of the knowledge of some student, one can op-\ntimize instruction. Existing methods are either based on tem-\nporal latent variable models, or factor analysis with temporal\nfeatures. We here show that factorization machines (FMs), a\nmodel for regression or classification, encompasses several\nexisting models in the educational literature as special cases,\nnotably additive factor model, performance factor model, and\nmultidimensional item response theory. We show, using sev-\neral real datasets of tens of thousands of users and items, that\nFMs can estimate student knowledge accurately and fast even\nwhen student data is sparsely observed, and handle side infor-\nmation such as multiple knowledge components and number\nof attempts at item or skill level. Our approach allows to fit\nstudent models of higher dimension than existing models, and\nprovides a testbed to try new combinations of features in or-\nder to improve existing models.",
            "source": "AAAI[C]",
            "year": "2019",
            "paper_file_name": "2019-AAAI-Knowledge Tracing Machines- Factorization Machines for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Exercise-Enhanced Sequential Modeling for Student Performance Prediction",
            "abstract": "In online education systems, for offering proactive services to students (e.g., personalized exercise recommendation), a crucial demand is to predict student performance (e.g., scores) on future exercising activities. Existing prediction methods mainly exploit the historical exercising records of students, where each exercise is usually represented as the manually labeled knowledge concepts, and the richer information contained in the text descriptions of exercises is still underexplored. In this paper, we propose a novel Exercise-Enhanced Recurrent Neural Network (EERNN) framework for student performance prediction by taking full advantage of both student exercising records and the text of each exercise. Specifically, for modeling the student exercising process, we first design a bidirectional LSTM to learn each exercise representation from its text description without any expertise and information loss. Then, we propose a new LSTM architecture to trace student states (i.e., knowledge states) in their sequential exercising process with the combination of exercise representations. For making final predictions, we design two strategies under EERNN, i.e., EERNNM with Markov property and EERNNA with Attention mechanism. Extensive experiments on large-scale real-world data clearly demonstrate the effectiveness of EERNN framework. Moreover, by incorporating the exercise correlations, EERNN can well deal with the cold start problems from both student and exercise perspectives.",
            "source": "AAAI[C]",
            "year": "2018",
            "paper_file_name": "2018-AAAI-Exercise-Enhanced Sequential Modeling for Student Performance Prediction.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor",
            "abstract": "Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners' mastery levels of programming knowledge based on their coding activities, facilitating more effective and personalized programming education. However, current PKT studies primarily focus on the implicit relationship between code content and knowledge assessment, often overlooking two types of noise signals in long-term programming activities: unwanted signals from unrelated submissions and weak signals from minor modifications. This practical challenge significantly limits model performance and application. To address this issue, we propose Coda, a Code graph-based tuning adaptor designed to enhance existing PKT models by identifying and mitigating the impact of noise. Specifically, Coda first transforms the loose code sequences submitted by each learner into a compact code graph. By leveraging this code graph, unwanted signals can be identified from a semantic similarity perspective. We then apply a cluster-aware GCN to the code graph, which improves the discrimination of weak signals and enables their clustering for identification. Finally, a lightweight yet effective adaptor is incorporated into the PKT task through optimization with two noise feature-based constraints and a navigational regularization term, to correct knowledge states affected by noise. It is worth mentioning that the Coda framework is model-agnostic and can be adapted to most existing PKT solutions. Extensive experimental results on four real-world datasets demonstrate that Coda effectively performs the PKT task in the presence of noisy programming records, outperforming typical baselines.",
            "source": "SIGKDD[C]",
            "year": "2025",
            "paper_file_name": "2025-SIGKDD-Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor.pdf"
        },
        {
            "model_name": "EKT",
            "paper_title": "EKT: Exercise-Aware Knowledge Tracing for Student Performance Prediction",
            "abstract": "Abstract—For offering proactive services (e.g., personalized exercise recommendation) to the students in computer supported intelligent education, one of the fundamental tasks is predicting student performance (e.g., scores) on future exercises, where it is necessary to track the change of each student's knowledge acquisition during her exercising activities. Unfortunately, to the best of our knowledge, existing approaches can only exploit the exercising records of students, and the problem of extracting rich information existed in the materials (e.g., knowledge concepts, exercise content) of exercises to achieve both more precise prediction of student performance and more interpretable analysis of knowledge acquisition remains underexplored. To this end, in this paper, we present a holistic study of student performance prediction. To directly achieve the primary goal of performance prediction, we ﬁrst propose a general Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both student's exercising records and the text content of corresponding exercises. In EERNN, we simply summarize each student's state into an integrated vector and trace it with a recurrent neural network, where we design a bidirectional LSTM to learn the encoding of each exercise from its content. For making ﬁnal predictions, we design two implementations on the basis of EERNN with different prediction strategies, i.e., EERNNM with Markov property and EERNNA with Attention mechanism. Then, to explicitly track student's knowledge acquisition on multiple knowledge concepts, we extend EERNN to an explainable Exercise-aware Knowledge Tracing (EKT) framework by incorporating the knowledge concept information, where the student's integrated state vector is now extended to a knowledge state matrix. In EKT, we further develop a memory network for quantifying how much each exercise can affect the mastery of students on multiple knowledge concepts during the exercising process. Finally, we conduct extensive experiments and evaluate both EERNN and EKT frameworks on a large-scale real-world data. The results in both general and cold-start scenarios clearly demonstrate the effectiveness of two frameworks in student performance prediction as well as the superior interpretability of EKT.",
            "source": "TOIS[T]",
            "year": "2021",
            "paper_file_name": "2021-TOIS-EKT@@Exercise-Aware Knowledge Tracing for Student Performance Prediction.pdf"
        },
        {
            "model_name": "DTransformer",
            "paper_title": "Tracing Knowledge Instead of Paterns: Stable Knowledge Tracing with Diagnostic Transformer",
            "abstract": "Knowledge Tracing (KT) aims at tracing the evolution of the knowledge states along the learning process of a learner. It has become a crucial task for online learning systems to model the learning process of their users, and further provide their users a personalized learning guidance. However, recent developments in KT based on deep neural networks mostly focus on increasing the accuracy of predicting the next performance of students. We argue that current KT modeling, as well as training paradigm, can lead to models tracing patterns of learner's learning activities, instead of their evolving knowledge states. In this paper, we propose a new architecture, Diagnostic Transformer (DTransformer), along with a new training paradigm, to tackle this challenge. With DTransformer, we build the architecture from question-level to knowledge-level, explicitly diagnosing learner's knowledge profciency from each question mastery states. We also propose a novel training algorithm based on contrastive learning that focuses on maintaining the stability of the knowledge state diagnosis. Through extensive experiments, we will show that with its understanding of knowledge state evolution, DTransformer achieves a better performance prediction accuracy and more stable knowledge state tracing results. We will also show that DTransformer is less sensitive to specifc patterns with case study. We open-sourced our code and data at https://github.com/yxonic/DTransformer.",
            "source": "WWW[C]",
            "year": "2023",
            "paper_file_name": "2023-WWW-DTransformer@@.pdf"
        },
        {
            "model_name": "CoKT",
            "paper_title": "Improving Knowledge Tracing with Collaborative Information",
            "abstract": "Knowledge tracing, which estimates students' knowledge states by predicting the probability that they correctly answer questions, is an essential task for online learning platforms. It has gained much attention in the decades due to its importance to downstream tasks like learning material arrangement, etc. The previous deep learning-based methods trace students' knowledge states with the explicitly intra-student information, i.e., they only consider the historical information of individuals to make predictions. However, they neglect the inter-student information, which contains the response correctness of other students who have similar question-answering experiences, may offer some valuable clues. Based on this consideration, we propose a method called Collaborative Knowledge Tracing (CoKT) in this paper, which sufficiently exploits the inter-student information in knowledge tracing. It retrieves the sequences of peer students who have similar question-answering experiences to obtain the inter-student information, and integrates the inter-student information with the intra-student information to trace students' knowledge states and predict their correctness in answering questions. We validate the effectiveness of our method on four real-world datasets and compare it with 10 baselines. The experimental results reveal that CoKT achieves the best performance.",
            "source": "WSDM[C]",
            "year": "2022",
            "paper_file_name": "2022-WSDM-CoKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Efﬁcient Attentive Knowledge Tracing for Long-Tail Distributed Records",
            "abstract": "Knowledge Tracing (KT), the most basic task for Intelligent Tutoring Systems, models individual students' knowledge state over time based on their past interactions. In large-scale KT datasets, we observe the length of student interaction records satisfy a long-tail distribution, and propose an efﬁcient self-attentive architecture, EAKT, optimized for this situation to accelerate training. By combining linear self-attention with adaptive window size, EAKT enables the long-range modeling capability for long records and high window utilization for short records. Through experiments on two large-scale KT datasets, we demonstrate that EAKT achieves about 4 times faster with one-third memory cost than the best existing model, while still maintains competitive accuracies.",
            "source": "ICBD[C]",
            "year": "2021",
            "paper_file_name": "2021-ICBD-Efficient Attentive Knowledge Tracing for Long-Tail Distributed Records.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Interpretable Knowledge Tracing with Difficulty-Aware Attention and Selective State Space Model",
            "abstract": "Knowledge Tracing (KT) aims to model students' knowledge states\nbased on their historical learning sequence, playing a critical role in\nonline education platforms. As the performance of sequence-based\nKT methods continues to improve, their increasing model complex-\nity and lack of transparency have become significant limitations.\nIn contrast, educational theory-driven KT methods incorporate\neducationally meaningful features (such as question difficulty or\ntime spent on questions) to enhance interpretability and perfor-\nmance. However, these models typically adopt simpler structures\nto reduce complexity and avoid overfitting, which limits their abil-\nity to effectively capture the sequential characteristics of learning\ncompared to sequence-based methods. To address these limitations,\nthis paper aims to integrate the strengths of both types of methods\nby proposing an Interpretable KT approach with Difficulty-Aware\nAttention and Selective State Space Model (ASIKT). Specifically,\nleveraging educational context, we design a difficulty-enhanced at-\ntention mechanism to model students' knowledge retrieval process\nat a fine-grained level without introducing additional parameters.\nMoreover, we employ selective state space model to capture the\ndynamic evolution of students' knowledge states based on their\nhistorical performance, ensuring both efficiency and simplicity\nin the model. Experimental results on four real-world datasets\ndemonstrate that ASIKT outperforms state-of-the-art KT methods,\nachieving superior predictive performance. Additionally, ASIKT\nprovides explanations for its predictions from multiple perspec-\ntives, showcasing strong interpretability. The code can be found at\nhttps://github.com/mrsser/ASIKT.",
            "source": "SIGIR[C]",
            "year": "2025",
            "paper_file_name": "2025-SIGIR-Interpretable Knowledge Tracing with Difficulty-Aware Attention and Selective State Space Model.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Interpretable Personalized Knowledge Tracing and Next Learning Activity Recommendation",
            "abstract": "Online learning systems that provide actionable and personalized guidance can help learners make better decisions during learning. Bayesian Knowledge Tracing (BKT) extensions [2] and deep learning based approaches have demonstrated improved mastery prediction accuracy compared to the basic BKT model; however, neither set of models provides actionable guidance on learning activities beyond mastery prediction. We propose a novel framework for personalized knowledge tracing with attention mechanism. Our proposed framework incorporates auxiliary learner attributes into knowledge tracing and interprets mastery prediction with the learning attributes. The proposed approach can also provide personalized next best learning activity recommendations. We demonstrate that the accuracy of the proposed approach in mastery prediction is slightly higher compared to deep learning based approaches and that the proposed approach can provide personalized next best learning activity recommendation.",
            "source": "L@S[C]",
            "year": "2020",
            "paper_file_name": "2020-L@S-Interpretable Personalized Knowledge Tracing and Next Learning Activity Recommendation.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Knowledge Tracing and Dynamic Student Classiﬁcation for Knowledge Tracing",
            "abstract": "Abstract—In Intelligent Tutoring System (ITS), tracing the\nstudent's knowledge state during learning has been studied for\nseveral decades in order to provide more supportive learning in-\nstructions. In this paper, we propose a novel model for knowledge\ntracing that i) captures students' learning ability and dynamically\nassigns students into distinct groups with similar ability at regular\ntime intervals, and ii) combines this information with a Recurrent\nNeural Network architecture known as Deep Knowledge Tracing.\nExperimental results conﬁrm that the proposed model is signiﬁ-\ncantly better at predicting student performance than well known\nstate-of-the-art techniques for student modelling.",
            "source": "ICDM[C]",
            "year": "2018",
            "paper_file_name": "2018-ICDM-Deep_Knowledge_Tracing_and_Dynamic_Student_Classification_for_Knowledge_Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Sequencing Educational Content in Classrooms using Bayesian Knowledge Tracing",
            "abstract": "Despite the prevalence of e-learning systems in schools, most of today's systems do not personalize educational data to the individual needs of each student. This paper proposes a new algorithm for sequencing questions to students that is empirically shown to lead to better performance and engagement in real schools when compared to a baseline approach. It is based on using knowledge tracing to model students' skill acquisition over time, and to select questions that advance the student's learning within the range of the student's capabilities, as determined by the model. The algorithm is based on a Bayesian Knowledge Tracing (BKT) model that incorporates partial credit scores, reasoning about multiple attempts to solve problems, and integrating item difficulty. This model is shown to outperform other BKT models that do not reason about (or reason about some but not all) of these features. The model was incorporated into a sequencing algorithm and deployed in two classes in different schools where it was compared to a baseline sequencing algorithm that was designed by pedagogical experts. In both classes, students using the BKT sequencing approach solved more difficult questions and attributed higher performance than did students who used the expert-based approach. Students were also more engaged using the BKT approach, as determined by their interaction time and number of log-ins to the system, as well as their reported opinion. We expect our approach to inform the design of better methods for sequencing and personalizing educational content to students that will meet their individual learning needs.",
            "source": "LAK[C]",
            "year": "2016",
            "paper_file_name": "2016-LAK-Sequencing Educational Content in Classrooms using Bayesian Knowledge Tracing.pdf"
        },
        {
            "model_name": "FDKT",
            "paper_title": "FDKT: Towards an Interpretable Deep Knowledge Tracing via Fuzzy Reasoning",
            "abstract": "In educational data mining, knowledge tracing (KT) aims to model learning performance based on student knowledge mastery. Deep-learning-based KT models perform remarkably better than traditional KT and have attracted considerable attention. However, most of them lack interpretability, making it challenging to explain why the model performed well in the prediction. In this paper, we propose an interpretable deep KT model, referred to as fuzzy deep knowledge tracing (FDKT) via fuzzy reasoning. Specifically, we formalize continuous scores into several fuzzy scores using the fuzzification module. Then, we input the fuzzy scores into the fuzzy reasoning module (FRM). FRM is designed to deduce the current cognitive ability, based on which the future performance was predicted. FDKT greatly enhanced the intrinsic interpretability of deep-learning-based KT through the interpretation of the deduction of student cognition. Furthermore, it broadened the application of KT to continuous scores. Improved performance with regard to both the advantages of FDKT was demonstrated through comparisons with the state-of-the-art models.",
            "source": "TOIS[T]",
            "year": "2024",
            "paper_file_name": "2024-TOIS-FDKT@@Towards an Interpretable Deep Knowledge Tracing via Fuzzy Reasoning.pdf"
        },
        {
            "model_name": "RIGL",
            "paper_title": "RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes",
            "abstract": "In the realm of education, both independent learning and group\nlearning are esteemed as the most classic paradigms. The former\nallows learners to self-direct their studies, while the latter is typi-\ncally characterized by teacher-directed scenarios. Recent studies\nin the field of intelligent education have leveraged deep tempo-\nral models to trace the learning process, capturing the dynamics\nof students' knowledge states, and have achieved remarkable per-\nformance. However, existing approaches have primarily focused\non modeling the independent learning process, with the group\nlearning paradigm receiving less attention. Moreover, the recip-\nrocal effect between the two learning processes, especially their\ncombined potential to foster holistic student development, remains\ninadequately explored. To this end, in this paper, we propose RIGL,\na unified Reciprocal model to trace knowledge states at both the\nindividual and group levels, drawing from the Independent and\nGroup Learning processes. Specifically, we first introduce a time\nframe-aware reciprocal embedding module to concurrently model\nboth student and group response interactions across various time\nframes. Subsequently, we employ reciprocal enhanced learning\nmodeling to fully exploit the comprehensive and complementary\ninformation between the two behaviors. Furthermore, we design a\nrelation-guided temporal attentive network, comprised of dynamic\ngraph modeling coupled with a temporal self-attention mechanism.\nIt is used to delve into the dynamic influence of individual and\ngroup interactions throughout the learning processes, which is\ncrafted to explore the dynamic intricacies of both individual and\ngroup interactions during the learning sequences. Conclusively,\nwe introduce a bias-aware contrastive learning module to bolster\nthe stability of the model's training. Extensive experiments on\nfour real-world educational datasets clearly demonstrate the effec-\ntiveness of the proposed RIGL model. Our codes are available at\nhttps://github.com/LabyrinthineLeo/RIGL.",
            "source": "SIGKDD[C]",
            "year": "2024",
            "paper_file_name": "2024-SIGKDD-RIGL@@A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes.pdf"
        },
        {
            "model_name": "AKT",
            "paper_title": "Context-Aware Attentive Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) refers to the problem of predicting future\nlearner performance given their past performance in educational\napplications. Recent developments in KT using flexible deep neural\nnetwork-based models excel at this task. However, these models of-\nten offer limited interpretability, thus making them insufficient for\npersonalized learning, which requires using interpretable feedback\nand actionable recommendations to help learners achieve better\nlearning outcomes. In this paper, we propose attentive knowledge\ntracing (AKT), which couples flexible attention-based neural net-\nwork models with a series of novel, interpretable model compo-\nnents inspired by cognitive and psychometric models. AKT uses\na novel monotonic attention mechanism that relates a learner's\nfuture responses to assessment questions to their past responses;\nattention weights are computed using exponential decay and a\ncontext-aware relative distance measure, in addition to the sim-\nilarity between questions. Moreover, we use the Rasch model to\nregularize the concept and question embeddings; these embeddings\nare able to capture individual differences among questions on the\nsame concept without using an excessive number of parameters.\nWe conduct experiments on several real-world benchmark datasets\nand show that AKT outperforms existing KT methods (by up to 6%\nin AUC in some cases) on predicting future learner responses. We\nalso conduct several case studies and show that AKT exhibits excel-\nlent interpretability and thus has potential for automated feedback\nand personalization in real-world educational settings.",
            "source": "SIGKDD[C]",
            "year": "2020",
            "paper_file_name": "2020-SIGKDD-AKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Federated Deep Knowledge Tracing",
            "abstract": "Knowledge tracing is a fundamental task in intelligent education for\ntracking the knowledge states of students on necessary concepts.\nIn recent years, Deep Knowledge Tracing (DKT) utilizes recurrent\nneural networks to model student learning sequences. This ap-\nproach has achieved significant success and has been widely used\nin many educational applications. However, in practical scenarios,\nit tends to suffer from the following critical problems due to data\nisolation: 1) Data scarcity. Educational data, which is usually dis-\ntributed across different silos (e.g., schools), is difficult to gather.\n2) Different data quality. Students in different silos have different\nlearning schedules, which results in unbalanced learning records,\nmeaning that it is necessary to evaluate the learning data quality in-\ndependently for different silos. 3) Data incomparability. It is difficult\nto compare the knowledge states of students with different learning\nprocesses from different silos. Inspired by federated learning, in\nthis paper, we propose a novel Federated Deep Knowledge Tracing\n(FDKT) framework to collectively train high-quality DKT models\nfor multiple silos. In this framework, each client takes charge of\ntraining a distributed DKT model and evaluating data quality by\nleveraging its own local data, while a center server is responsible\nfor aggregating models and updating the parameters for all the\nclients. In particular, in the client part, we evaluate data quality\nincorporating different education measurement theories, and we\nconstruct two quality-oriented implementations based on FDKT,\ni.e., FDKTCTT and FDKTIRT-where the means of data quality eval-\nuation follow Classical Test Theory and Item Response Theory,\nrespectively. Moreover, in the server part, we adopt hierarchical\nmodel interpolation to uptake local effects for model personaliza-\ntion. Extensive experiments on real-world datasets demonstrate the\neffectiveness and superiority of the FDKT framework.",
            "source": "WSDM[C]",
            "year": "2021",
            "paper_file_name": "2021-WSDM-Federated Deep Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Knowledge Tracing with Sequential Key-Value Memory Networks",
            "abstract": "Can machines trace human knowledge like humans? Knowledge tracing (KT) is a fundamental task in a wide range of applications in education, such as massive open online courses (MOOCs), intelligent tutoring systems, educational games, and learning management systems. It models dynamics in a student's knowledge states in relation to different learning concepts through their interactions with learning activities. Recently, several attempts have been made to use deep learning models for tackling the KT problem. Although these deep learning models have shown promising results, they have limitations: either lack the ability to go deeper to trace how specific concepts in a knowledge state are mastered by a student, or fail to capture long-term dependencies in an exercise sequence. In this paper, we address these limitations by proposing a novel deep learning model for knowledge tracing, namely Sequential Key-Value Memory Networks (SKVMN). This model unifies the strengths of recurrent modelling capacity and memory capacity of the existing deep learning KT models for modelling student learning. We have extensively evaluated our proposed model on five benchmark datasets. The experimental results show that (1) SKVMN outperforms the state-of-the-art KT models on all datasets, (2) SKVMN can better discover the correlation between latent concepts and questions, and (3) SKVMN can trace the knowledge state of students dynamics, and a leverage sequential dependencies in an exercise sequence for improved predication accuracy.",
            "source": "SIGIR[C]",
            "year": "2019",
            "paper_file_name": "2019-SIGIR-Knowledge Tracing with Sequential Key-Value Memory Networks.pdf"
        },
        {
            "model_name": "CL4KT",
            "paper_title": "Contrastive Learning for Knowledge Tracing",
            "abstract": "Knowledge tracing is the task of understanding student's knowledge acquisition processes by estimating whether to solve the next question correctly or not. Most deep learning-based methods tackle this problem by identifying hidden representations of knowledge states from learning histories. However, due to the sparse interactions between students and questions, the hidden representations can be easily over-fitted and often fail to capture student's knowledge states accurately. This paper introduces a contrastive learning framework for knowledge tracing that reveals semantically similar or dissimilar examples of a learning history and stimulates to learn their relationships. To deal with the complexity of knowledge acquisition during learning, we carefully design the components of contrastive learning, such as architectures, data augmentation methods, and hard negatives, taking into account pedagogical rationales. Our extensive experiments on six benchmarks show statistically significant improvements from the previous methods. Further analysis shows how our methods contribute to improving knowledge tracing performances.",
            "source": "WWW[C]",
            "year": "2022",
            "paper_file_name": "2022-WWW-CL4KT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Toward the Evaluation of Educational Videos using Bayesian Knowledge Tracing and Big Data",
            "abstract": "Along with the advent of MOOCs and other online\nlearning platforms such as Khan Academy, the role of\nonline education has continued to grow in relation to that\nof traditional on-campus instruction. Rather than tackle\nthe problem of evaluating large educational units such as\nentire online courses, this paper approaches a smaller\nproblem: exploring a framework for evaluating more\nganular educational units, in this case, short educational\nvideos. We have chosen to leverage an adaptation of\ntraditional Bayesian Knowledge Tracing (BKT), intended\nto incorporate the usage of video content in addition to\nassessment activity. By exploring the change in predictive\nerror when alternately including or omitting video activity,\nwe suggest a metric for determining the relevance of\nvideos to associated assessments. To validate our\nhypothesis and demonstrate the application of our\nproposed methods we use data obtained from the popular\nKhan Academy website.",
            "source": "L@S[C]",
            "year": "2015",
            "paper_file_name": "2015-L@S-Toward the Evaluation of Educational Videos using Bayesian Knowledge Tracing and Big Data.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Automatic RNN Cell Design for Knowledge Tracing using Reinforcement Learning",
            "abstract": "Empirical results have shown that deep neural networks\nachieve superior performance in the application of Knowledge\nTracing. However, the design of recurrent cells like long short\nterm memory (LSTM) cells or gated recurrent units (GRU)\nis inﬂuenced largely by applications in natural language pro-\ncessing. They were proposed and evaluated in the context\nof sequence to sequence modeling, like machine translation.\nEven though the LSTM cell works well for knowledge tracing,\nit is unknown if its architecture is ideally suited for knowledge\ntracing. Despite the fact that there are several recurrent neural\nnetwork based architectures proposed for knowledge tracing,\nthe methodologies rely on empirical observations and trial and\nerror, which may not be efﬁcient or scalable. In this study,\nwe investigate using reinforcement learning for the automatic\ndesign of recurrent neural network cells for knowledge tracing,\nshowing improved performance compared to the LSTM cell.\nWe also discuss a potential method for model regularization\nusing neural architecture search.",
            "source": "L@S[C]",
            "year": "2020",
            "paper_file_name": "2020-L@S-Automatic RNN Cell Design for Knowledge Tracing using Reinforcement Learning.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Enhancing Knowledge Tracing through Decoupling Cognitive Pattern from Error-Prone Data",
            "abstract": "Knowledge tracing (KT) aims to predict students' future performance based on their past learning activities. However, no one is perfect. Factors such as carelessness, fatigue, and stress often cause students to make mistakes on problems they have already mastered, leading to anomalies in their historical learning data. These anomalies disrupt inherent patterns in the data, misleading the KT model. Extracting cognitive patterns that accurately reflect students' knowledge mastery from such error-prone data remains a significant challenge. Against this background, this paper proposes a novel KT method named RoubstKT1, inspired by educational measurement theory and frequency-based decomposition. A cognitive decoupling analyzer is proposed to decouple the student's cognitive pattern and random factors from the data through smoothing and subtraction operations, then recombine them using a gating mechanism or adaptive parameter fusion strategy. To more effectively diagnose students' knowledge mastery, we employ a decay-based attention mechanism that focuses on random behaviors at adjacent time steps. We conducted comprehensive experiments based on real-world datasets and targeted datasets with added random noise. The experimental results demonstrated the effectiveness of the proposed method.",
            "source": "WWW[C]",
            "year": "2025",
            "paper_file_name": "2025-WWW-Enhancing Knowledge Tracing through Decoupling Cognitive Pattern from Error-Prone Data.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Predictive Student Modelling in an Online Reading Platform",
            "abstract": "Use of technology-enhanced education and online learning\nsystems has become more popular, especially since the onset\nof the COVID-19 pandemic. These systems capture a rich ar-\nray of data as students interact with them. Predicting student\nperformance is an essential part of technology-enhanced ed-\nucation systems to enable the generation of hints and provide\nrecommendations to students. Typically, this is done through\nuse of data on student interactions with questions without uti-\nlizing important data on the temporal ordering of students'\nother interaction behavior, (e.g., reading, video watching).\nIn this paper, we hypothesize that to predict students' ques-\ntion performance, it is necessary to (i) consider other learn-\ning activities beyond question-answering and (ii) understand\nhow these activities are related to question-solving behavior.\nWe collected middle school physical science students' data\nwithin a K12 reading platform, Actively Learn. This platform\nprovides reading-support to students and collects trace data\non their use of the system. We propose a transformer-based\nmodel to predict students' question scores utilizing question\ninteraction and reading-related behaviors. Our findings show\nthat integrating question attempts and reading-related behav-\niors results in better predictive power compared to using only\nquestion attempt features. The interpretable visualization of\ntransformer's attention can be helpful for teachers to make\ntailored interventions in students' learning.",
            "source": "AAAI[C]",
            "year": "2022",
            "paper_file_name": "2022-AAAI-Predictive Student Modelling in an Online Reading Platform-close.pdf"
        },
        {
            "model_name": "",
            "paper_title": "XES3G5M: A Knowledge Tracing Benchmark Dataset with Auxiliary Information",
            "abstract": "Knowledge tracing (KT) is a task that predicts students' future performance based on their historical learning interactions. With the rapid development of deep learning techniques, existing KT approaches follow a data-driven paradigm that uses massive problem-solving records to model students' learning processes. However, although the educational contexts contain various factors that may have an influence on student learning outcomes, existing public KT datasets mainly consist of anonymized ID-like features, which may hinder the research advances towards this field. Therefore, in this work, we present, XES3G5M, a large-scale dataset with rich auxiliary information about questions and their associated knowledge components (KCs)2. The XES3G5M dataset is collected from a real-world online math learning platform, which contains 7,652 questions, and 865 KCs with 5,549,635 interactions from 18,066 students. To the best of our knowledge, the XES3G5M dataset not only has the largest number of KCs in math domain but contains the richest contextual information including tree structured KC relations, question types, textual contents and analysis and student response timestamps. Furthermore, we build a comprehensive benchmark on 19 state-of-the-art deep learning based knowledge tracing (DLKT) models. Extensive experiments demonstrate the effectiveness of leveraging the auxiliary information in our XES3G5M with DLKT models. We hope the proposed dataset can effectively facilitate the KT research work.",
            "source": "NeurIPS[C]",
            "year": "2023",
            "paper_file_name": "2023-NeurIPS-XES3G5M- A Knowledge Tracing Benchmark Dataset with Auxiliary Information.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Knowledge Tracing with Transformers",
            "abstract": "Abstract. In this work, we propose a Transformer-based model to trace\nstudents' knowledge acquisition. We modiﬁed the Transformer struc-\nture to utilize 1) the association between questions and skills and 2)\nthe elapsed time between question steps. The use of question-skill asso-\nciations allows the model to learn speciﬁc representation for frequently\nencountered questions while representing rare questions with their under-\nline skill representations. The inclusion of elapsed time opens the oppor-\ntunity to address forgetting. Our approach outperforms the state-of-the-\nart methods in the literature by roughly 10% in AUC with frequently\nused public datasets.",
            "source": "AIED[C]",
            "year": "2020",
            "paper_file_name": "2020-AIED-Deep Knowledge Tracing with Transformers.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Fairer but Not Fair Enough On the Equitability of Knowledge Tracing",
            "abstract": "Adaptive educational technologies have the capacity to meet the\nneeds of individual students in theory, but in some cases, the de-\ngree of personalization might be less than desired, which could\nlead to inequitable outcomes for students. In this paper, we use\nsimulations to demonstrate that while knowledge tracing algo-\nrithms are substantially more equitable than giving all students the\nsame amount of practice, such algorithms can still be inequitable\nwhen they rely on inaccurate models. This can arise as a result\nof two factors: (1) using student models that are fit to aggregate\npopulations of students, and (2) using student models that make\nincorrect assumptions about student learning. In particular, we\ndemonstrate that both the Bayesian knowledge tracing algorithm\nand the N-Consecutive Correct Responses heuristic are susceptible\nto these concerns, but that knowledge tracing with the additive\nfactor model may be more equitable. The broader message of this\npaper is that when designing learning analytics algorithms, we\nneed to explicitly consider whether the algorithms act fairly with\nrespect to different populations of students, and if not, how we can\nmake our algorithms more equitable.",
            "source": "LAK[C]",
            "year": "2019",
            "paper_file_name": "2019-LAK-Fairer but Not Fair Enough On the Equitability of Knowledge Tracing.pdf"
        },
        {
            "model_name": "DKT",
            "paper_title": "Deep Knowledge Tracing",
            "abstract": "Knowledge tracing—where a machine models the knowledge of a student as they\ninteract with coursework—is a well established problem in computer supported\neducation. Though effectively modeling student knowledge would have high ed-\nucational impact, the task has many inherent challenges. In this paper we explore\nthe utility of using Recurrent Neural Networks (RNNs) to model student learning.\nThe RNN family of models have important advantages over previous methods\nin that they do not require the explicit encoding of human domain knowledge,\nand can capture more complex representations of student knowledge. Using neu-\nral networks results in substantial improvements in prediction performance on a\nrange of knowledge tracing datasets. Moreover the learned model can be used for\nintelligent curriculum design and allows straightforward interpretation and dis-\ncovery of structure in student tasks. These results suggest a promising new line of\nresearch for knowledge tracing and an exemplary application task for RNNs.",
            "source": "NeurIPS[C]",
            "year": "2015",
            "paper_file_name": "2015-NeurIPS-DKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Dynamic Student Classiﬃcation on Memory Networks for Knowledge Tracing",
            "abstract": "Abstract. Knowledge Tracing (KT) is the assessment of student's\nknowledge state and predicting whether that student may or may not\nanswer the next problem correctly based on a number of previous prac-\ntices and outcomes in their learning process. KT leverages machine learn-\ning and data mining techniques to provide better assessment, supportive\nlearning feedback and adaptive instructions. In this paper, we propose a\nnovel model called Dynamic Student Classiﬁcation on Memory Networks\n(DSCMN) for knowledge tracing that enhances existing KT approaches\nby capturing temporal learning ability at each time interval in student's\nlong-term learning process. Experimental results conﬁrm that the pro-\nposed model is signiﬁcantly better at predicting student performance\nthan well known state-of-the-art KT modelling techniques.",
            "source": "PAKDD[C]",
            "year": "2019",
            "paper_file_name": "2019-PAKDD-Dynamic Student Classiffication on Memory Networks for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Disentangled Knowledge Tracing for Alleviating Cognitive Bias",
            "abstract": "In the realm of Intelligent Tutoring System (ITS), the accurate assessment of students' knowledge states through Knowledge Tracing (KT) is crucial for personalized learning. However, due to data bias, i.e., the unbalanced distribution of question groups (e.g., concepts), conventional KT models are plagued by cognitive bias, which tends to result in cognitive underload for overperformers and cognitive overload for underperformers. More seriously, this bias is amplified with the exercise recommendations by ITS. After delving into the causal relations in the KT models, we identify the main cause as the confounder effect of students' historical correct rate distribution over question groups on the student representation and prediction score. Towards this end, we propose a Disentangled Knowledge Tracing (DisKT) model, which separately models students' familiar and unfamiliar abilities based on causal effects and eliminates the impact of the confounder in student representation within the model. Additionally, to shield the contradictory psychology (e.g., guessing and mistaking) in the students' biased data, DisKT introduces a contradiction attention mechanism. Furthermore, DisKT enhances the interpretability of the model predictions by integrating a variant of Item Response Theory. Experimental results on 11 benchmarks and 3 synthesized datasets with different bias strengths demonstrate that DisKT significantly alleviates cognitive bias and outperforms 16 baselines in evaluation accuracy.",
            "source": "WWW[C]",
            "year": "2025",
            "paper_file_name": "2025-WWW-Disentangled Knowledge Tracing for Alleviating Cognitive Bias.pdf"
        },
        {
            "model_name": "LPKT",
            "paper_title": "Learning Process-consistent Knowledge Tracing",
            "abstract": "Knowledge tracing (KT), which aims to trace students' changing knowledge state during their learning process, has improved students' learning efficiency in online learning systems. Recently, KT has attracted much research attention due to its critical significance in education. However, most of the existing KT methods pursue high accuracy of student performance prediction but neglect the consistency of students' changing knowledge state with their learning process. In this paper, we explore a new paradigm for the KT task and propose a novel model named Learning Process-consistent Knowledge Tracing (LPKT), which monitors students' knowledge state through directly modeling their learning process. Specifically, we first formalize the basic learning cell as the tuple exercise—answer time—answer. Then, we deeply measure the learning gain as well as its diversity from the difference of the present and previous learning cells, their interval time, and students' related knowledge state. We also design a learning gate to distinguish students' absorptive capacity of knowledge. Besides, we design a forgetting gate to model the decline of students' knowledge over time, which is based on their previous knowledge state, present learning gains, and the interval time. Extensive experimental results on three public datasets demonstrate that LPKT could obtain more reasonable knowledge state in line with the learning process. Moreover, LPKT also outperforms state-of-the-art KT methods on student performance prediction. Our work indicates a potential future research direction for KT, which is of both high interpretability and accuracy.",
            "source": "SIGKDD[C]",
            "year": "2021",
            "paper_file_name": "2021-SIGKDD-LPKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing",
            "abstract": "In this paper, we propose a novel Transformer-based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT has an encoder-decoder structure where the exercise and response embedding sequences separately enter, respectively, the encoder and the decoder. The encoder applies self-attention layers to the sequence of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to the sequence of response embeddings. This separation of input allows us to stack attention layers multiple times, resulting in an improvement in area under receiver operating characteristic curve (AUC). To the best of our knowledge, this is the ﬁrst work to suggest an encoder-decoder model for knowledge tracing that applies deep self-attentive layers to exercises and responses separately. We empirically evaluate SAINT on a large-scale knowledge tracing dataset, EdNet, collected by an active mobile education application, Santa, which has 627,347 users, 72,907,005 response data points as well as a set of 16,175 exercises gathered since 2016. The results show that SAINT achieves state-of-the-art performance in knowledge tracing with an improvement of 1.8% in AUC compared to the current state-of-the-art model.",
            "source": "L@S[C]",
            "year": "2020",
            "paper_file_name": "2020-L@S-Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Counterfactual Monotonic Knowledge Tracing for Assessing Students’ Dynamic Mastery of Knowledge Concepts",
            "abstract": "As the core of the Knowledge Tracking (KT) task, assessing students’ dynamic mastery of knowledge concepts is crucial for both offline teaching and online educational applications. Since students’ mastery of knowledge concepts is often unlabeled, existing KT methods focus on predicting students’ responses to practices. However, purely predicting student responses without imposing specific constraints on hidden concept mastery values does not guarantee the accuracy of these intermediate values as concept mastery values. To address this issue, we propose a principled approach called Counterfactual Monotonic Knowledge Tracing (CMKT), which builds on the implicit paradigm described above by using a counterfactual assumption to constrain the evolution of students’ mastery of knowledge concepts. Specifically, CMKT first assesses students’ knowledge concept mastery value based on their historical practice sequences. Then, CMKT sets the answer of the most recent practice as the opposite of the actual answer and, based on this counterfactual answer, assesses the student’s corresponding counterfactual knowledge mastery value. During the model training process, CMKT constrains the update of the student’s knowledge states by ensuring that the two types of knowledge mastery values of students satisfy a fundamental educational theory, the monotonicity theory, to provide specific semantics for the assessed mastery values by the model. Finally, extensive experiments on five datasets demonstrate the superiority of CMKT over baseline models.",
            "source": "CIKM[C]",
            "year": "2023",
            "paper_file_name": "2023-CIKM-Counterfactual Monotonic Knowledge Tracing for Assessing Students’ Dynamic Mastery of Knowledge Concepts.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Online Deep Knowledge Tracing",
            "abstract": "Abstract—This study focuses on solving the problem of knowl-edge tracing in a practical situation, where the responses from\nstudents come in a stream. Most current works of deep knowledge\ntracing are pursuing to integrate of more side information or data\nstructure, but they often fail to make self-update in the dynamic\nlearning situation. Towards this end, we here proposed an online\ndeep knowledge tracing model, dubbed ODKT, by utilizing the\nonline gradient descent algorithm to develop the traditional\ndeep knowledge tracing (DKT) into online learning. Rather than\nlearning a perfect model, the ODKT aims to train DKT in its\nusing process step by step. Experiments were conducted on four\npublic datasets for knowledge tracing. The results demonstrate\nthat the ODKT model is effective and more suitable for practical\napplications.",
            "source": "ICDM[C]",
            "year": "2022",
            "paper_file_name": "2022-ICDM-Online Deep Knowledge Tracing-close.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Towards Modeling Learner Performance with Large Language Models",
            "abstract": "Recent work exploring the capabilities of pre-trained large language models (LLMs) has demonstrated their ability to act as general pattern machines by completing complex token sequences representing a wide array of tasks, including time-series prediction and robot control. This paper investigates whether the pattern recognition and sequence modeling capabilities of LLMs can be extended to the domain of knowledge tracing, a critical component in the development of intelligent tutoring systems (ITSs) that tailor educational experiences by predicting learner performance over time. In an empirical evaluation across multiple real-world datasets, we compare two approaches to using LLMs for this task, zero-shot prompting and model fine-tuning, with existing, non-LLM approaches to knowledge tracing. While LLM-based approaches do not achieve state-of-the-art performance, fine-tuned LLMs surpass the performance of naive baseline models and perform on par with standard Bayesian Knowledge Tracing approaches across multiple metrics. These findings suggest that the pattern recognition capabilities of LLMs can be used to model complex learning trajectories, opening a novel avenue for applying LLMs to educational contexts. The paper concludes with a discussion of the implications of these findings for future research, suggesting that further refinements and a deeper understanding of LLMs' predictive mechanisms could lead to enhanced performance in knowledge tracing tasks1.",
            "source": "EDM[C]",
            "year": "2024",
            "paper_file_name": "2024-EDM-Towards Modeling Learner Performance with Large Language Models.pdf"
        },
        {
            "model_name": "KPT",
            "paper_title": "Tracking Knowledge Proficiency of Students with Educational Priors",
            "abstract": "Diagnosing students' knowledge proficiency, i.e., the mastery degrees of a particular knowledge point in exercises, is a crucial issue for numerous educational applications, e.g., targeted knowledge training and exercise recommendation. Educational theories have converged that students learn and forget knowledge from time to time. Thus, it is necessary to track their mastery of knowledge over time. However, traditional methods in this area either ignored the explanatory power of the diagnosis results on knowledge points or relied on a static assumption. To this end, in this paper, we devise an explanatory probabilistic approach to track the knowledge proficiency of students over time by leveraging educational priors. Specifically, we first associate each exercise with a knowledge vector in which each element represents an explicit knowledge point by leveraging educational priors (i.e., Q-matrix). Correspondingly, each student is represented as a knowledge vector at each time in a same knowledge space. Second, given the student knowledge vector over time, we borrow two classical educational theories (i.e., Learning curve and Forgetting curve) as priors to capture the change of each student's proficiency over time. After that, we design a probabilistic matrix factorization framework by combining student and exercise priors for tracking student knowledge proficiency. Extensive experiments on three real-world datasets demonstrate both the effectiveness and explanatory power of our proposed model.",
            "source": "CIKM[C]",
            "year": "2017",
            "paper_file_name": "2017-CIKM-KPT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Structure-based Knowledge Tracing: An Inﬂuence Propagation View",
            "abstract": "Abstract—Knowledge Tracing (KT) is a fundamental but challenging task in online education that traces learners' evolving knowledge states. Much attention has been drawn to this area and several works such as Bayesian and Deep Knowledge Tracing have been proposed. Recent works have explored the value of relations among concepts and proposed to introduce knowledge structure into KT tasks. However, the propagated inﬂuence among concepts, which has been shown to be a key factor in human learning by the educational theories, is still under-explored. In this paper, we propose a new framework called Structure-based Knowledge Tracing (SKT), which exploits the multiple relations in knowledge structure to model the inﬂuence propagation among concepts. In the SKT framework, we consider both the temporal effect on the exercising sequence and the spatial effect on the knowledge structure. We take advantages of two novel formulations in modeling the inﬂuence propagation on the knowledge structure with multiple relations. For undirected relations such as similarity relations, the synchronization propagation method is adopted, where the inﬂuence propagates bidirectionally between neighbor concepts. For directed relations such as prerequisite relations, the partial propagation method is applied, where the inﬂuence can only unidirectionally propagate from a predecessor to a successor. Meanwhile, we employ the gated functions to update the states of concepts temporally and spatially. We demonstrate the effectiveness and interpretability of SKT with extensive experiments.",
            "source": "ICDM[C]",
            "year": "2020",
            "paper_file_name": "2020-ICDM-Structure-Based_Knowledge_Tracing_An_Influence_Propagation_View.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Knowledge Tracing On Programming Exercises",
            "abstract": "Modeling a student's knowledge state while she is solving\nexercises is a crucial stepping stone towards providing bet-\nter personalized learning experiences at scale. This task,\nalso referred to as \"knowledge tracing\", has been explored\nextensively on exercises where student submissions fall\ninto a ﬁnite discrete solution space, e.g. a multiple-choice\nanswer. However, we believe that rich information about\na student's learning is captured within their responses\nto open-ended problems with unbounded solution spaces,\nsuch as programming exercises. In addition, sequential\nsnapshots of a student's progress while she is solving\na single exercise can provide valuable insights into her\nlearning behavior. In this setting, creating representa-\ntions for a student's knowledge state is a challenging task,\nbut with recent advances in machine learning, there are\nmore promising techniques to learn representations for\ncomplex entities. In our work, we feed the embedded\nprogram submissions into a recurrent neural network and\ntrain it on the task of predicting the student's success\non the subsequent programming exercise. By training on\nthis task, the model learns nuanced representations of a\nstudent's knowledge, and reliably predicts future student\nperformance.",
            "source": "L@S[C]",
            "year": "2017",
            "paper_file_name": "2017-L@S-Deep Knowledge Tracing On Programming Exercises.pdf"
        },
        {
            "model_name": "SAKT",
            "paper_title": "A Self-Attentive model for Knowledge Tracing",
            "abstract": "Knowledge tracing is the task of modeling each student's\nmastery of knowledge concepts (KCs) as (s)he engages with\na sequence of learning activities. Each student's knowledge\nis modeled by estimating the performance of the student\non the learning activities. It is an important research area\nfor providing a personalized learning platform to students.\nIn recent years, methods based on Recurrent Neural Net-\nworks (RNN) such as Deep Knowledge Tracing (DKT) and\nDynamic Key-Value Memory Network (DKVMN) outper-\nformed all the traditional methods because of their ability to\ncapture a complex representation of human learning. How-\never, these methods face the issue of not generalizing well\nwhile dealing with sparse data which is the case with real-\nworld data as students interact with few KCs. In order to\naddress this issue, we develop an approach that identiﬁes\nthe KCs from the student's past activities that are rele-\nvant to the given KC and predicts his/her mastery based\non the relatively few KCs that it picked. Since predictions\nare made based on relatively few past activities, it handles\nthe data sparsity problem better than the methods based\non RNN. For identifying the relevance between the KCs,\nwe propose a self-attention based approach, Self Attentive\nKnowledge Tracing (SAKT). Extensive experimentation on\na variety of real-world dataset shows that our model out-\nperforms the state-of-the-art models for knowledge tracing,\nimproving AUC by 4.43% on average.",
            "source": "EDM[C]",
            "year": "2019",
            "paper_file_name": "2019-EDM-SAKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Improving Model Fairness with Time-Augmented Bayesian Knowledge Tracing",
            "abstract": "Modelling student performance is an increasingly popular goal in the learning analytics community. A common method for this task is Bayesian Knowledge Tracing (BKT), which predicts student performance and topic mastery using the student's answer history. While BKT has strong qualities and good empirical performance, like many machine learning approaches it can be prone to bias. In this study we demonstrate an inherent bias in BKT with respect to students' income support levels and gender, using publicly available data. We find that this bias is likely a result of the model's 'slip' parameter disregarding answer speed when deciding if a student has lost mastery status. We propose a new BKT model variation that directly considers answer speed, resulting in a significant fairness increase without sacrificing model performance. We discuss the role of answer speed as a potential cause of BKT model bias, as well as a method to minimise bias in future implementations.",
            "source": "LAK[C]",
            "year": "2024",
            "paper_file_name": "2024-LAK-Improving Model Fairness with Time-Augmented Bayesian Knowledge Tracing.pdf"
        },
        {
            "model_name": "ELAKT",
            "paper_title": "ELAKT: Enhancing Locality for Attentive Knowledge Tracing",
            "abstract": "Knowledge tracing models based on deep learning can achieve impressive predictive performance by leveraging attention mechanisms. However, there still exist two challenges in attentive knowledge tracing (AKT): First, the mechanism of classical models of AKT demonstrates relatively low attention when processing exercise sequences with shifting knowledge concepts (KC), making it difficult to capture the comprehensive state of knowledge across sequences. Second, classical models do not consider stochastic behaviors, which negatively affects models of AKT in terms of capturing anomalous knowledge states. This article proposes a model of AKT, called Enhancing Locality for Attentive Knowledge Tracing (ELAKT), that is a variant of the deep KT model. The proposed model leverages the encoder module of the transformer to aggregate knowledge embedding generated by both exercises and responses over all timesteps. In addition, it uses causal convolutions to aggregate and smooth the states of local knowledge. The ELAKT model uses the states of comprehensive KCs to introduce a prediction correction module to forecast the future responses of students to deal with noise caused by stochastic behaviors. The results of experiments demonstrated that the ELAKT model consistently outperforms state-of-the-art baseline KT models.",
            "source": "TOIS[T]",
            "year": "2024",
            "paper_file_name": "2024-TOIS-ELAKT@@Enhancing Locality for Attentive Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Variational Deep Knowledge Tracing for Language Learning",
            "abstract": "Deep Knowledge Tracing (DKT), which traces a student's knowledge change using deep recurrent neural networks, is widely adopted in student cognitive modeling. Current DKT models only predict a student's performance based on the observed learning history. However, a student's learning processes often contain latent events not directly observable in the learning history, such as partial understanding, making slips, and guessing answers. Current DKT models fail to model this kind of stochasticity in the learning process. To address this issue, we propose Variational Deep Knowledge Tracing (VDKT), a latent variable DKT model that incorporates stochasticity into DKT through latent variables. We show that VDKT outperforms both a sequence-to-sequence DKT baseline and previous SoTA methods on MAE, F1, and AUC by evaluating our approach on two Duolingo language learning datasets. We also draw various interpretable analyses from VDKT and offer insights into students' stochastic behaviors in language learning.",
            "source": "LAK[C]",
            "year": "2021",
            "paper_file_name": "2021-LAK-Variational Deep Knowledge Tracing for Language Learning.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations",
            "abstract": "Knowledge tracing (KT) is a crucial technique to predict stu-\ndents' future performance by observing their historical learn-\ning processes. Due to the powerful representation ability of\ndeep neural networks, remarkable progress has been made\nby using deep learning techniques to solve the KT prob-\nlem. The majority of existing approaches rely on the ho-\nmogeneous question assumption that questions have equiv-\nalent contributions if they share the same set of knowledge\ncomponents. Unfortunately, this assumption is inaccurate in\nreal-world educational scenarios. Furthermore, it is very chal-\nlenging to interpret the prediction results from the existing\ndeep learning based KT models. Therefore, in this paper, we\npresent QIKT, a question-centric interpretable KT model to\naddress the above challenges. The proposed QIKT approach\nexplicitly models students' knowledge state variations at a\nﬁne-grained level with question-sensitive cognitive represen-\ntations that are jointly learned from a question-centric knowl-\nedge acquisition module and a question-centric problem solv-\ning module. Meanwhile, the QIKT utilizes an item response\ntheory based prediction layer to generate interpretable predic-\ntion results. The proposed QIKT model is evaluated on three\npublic real-world educational datasets. The results demon-\nstrate that our approach is superior on the KT prediction task,\nand it outperforms a wide range of deep learning based KT\nmodels in terms of prediction accuracy with better model\ninterpretability. To encourage reproducible results, we have\nprovided all the datasets and code at https://pykt.org/.",
            "source": "AAAI[C]",
            "year": "2023",
            "paper_file_name": "2023-AAAI-Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Towards Robust Knowledge Tracing Models via k-Sparse Attention",
            "abstract": "Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interaction sequences. With the advanced capability of capturing contextual long-term dependency, attention mechanism becomes one of the essential components in many deep learning based KT (DLKT) models. In spite of the impressive performance achieved by these attentional DLKT models, many of them are often vulnerable to run the risk of overfitting, especially on small-scale educational datasets. Therefore, in this paper, we propose sparseKT, a simple yet effective framework to improve the robustness and generalization of the attention based DLKT approaches. Specifically, we incorporate a k-selection module to only pick items with the highest attention scores. We propose two sparsification heuristics : (1) soft-thresholding sparse attention and (2) top-𝐾sparse attention. We show that our sparseKT is able to help attentional KT models get rid of irrelevant student interactions and improve the predictive performance when compared to 11 state-of-the-art KT models on three publicly available real-world educational datasets. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit1.",
            "source": "SIGIR[C]",
            "year": "2023",
            "paper_file_name": "2023-SIGIR-Towards Robust Knowledge Tracing Models via k-Sparse Attention.pdf"
        },
        {
            "model_name": "ABQR",
            "paper_title": "Adversarial Bootstrapped Question Representation Learning for Knowledge Tracing",
            "abstract": "Knowledge tracing (KT), which estimates and traces the degree of learners' mastery of concepts based on students' responses to learning resources, has become an increasingly relevant problem in intelligent education. The accuracy of predictions greatly depends on the quality of question representations. While contrastive learning has been commonly used to generate high-quality representations, the selection of positive and negative samples for knowledge tracing remains a challenge. To address this issue, we propose an adversarial bootstrapped question representation (ABQR) model, which can generate robust and high-quality question representations without requiring negative samples. Specifically, ABQR introduces the bootstrap self-supervised learning framework, which learns question representations from different views of the skill-informed question interaction graph and facilitates question representations between each view to predict one another, thereby circumventing the need for negative sample selection. Moreover, we propose a multi-objective multi-round feature adversarial graph augmentation method to obtain a higher-quality target view, while preserving the structural information of the original graph. ABQR is versatile and can be easily integrated with any base KT model as a plug-in to enhance the quality of question representation. Extensive experiments demonstrate that ABQR significantly improves the performance of the base KT model and outperforms state-of-the-art models. Ablation experiments confirm the effectiveness of each module of ABQR. The code is available at https://github.com/lilstrawberry/ABQR.",
            "source": "MM[C]",
            "year": "2023",
            "paper_file_name": "2023-MM-ABQR@@.pdf"
        },
        {
            "model_name": "DyGKT",
            "paper_title": "DyGKT: Dynamic Graph Learning for Knowledge Tracing",
            "abstract": "Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions. Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2) The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving. The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods. Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT. In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field. Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals. Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature. Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model. All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT.",
            "source": "SIGKDD[C]",
            "year": "2024",
            "paper_file_name": "2024-SIGKDD-DyGKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Improving Knowledge Tracing via Pre-training Question Embeddings",
            "abstract": "Knowledge tracing (KT) deﬁnes the task of pre-dicting whether students can correctly answer ques-tions based on their historical response. Although much research has been devoted to exploiting the question information, plentiful advanced informa-tion among questions and skills hasn't been well extracted, making it challenging for previous work to perform adequately. In this paper, we demonstrate that large gains on KT can be realized by pre-training embeddings for each question on abundant side information, followed by training deep KT models on the obtained embeddings. To be speciﬁc, the side information includes question dif-ﬁculty and three kinds of relations contained in a bipartite graph between questions and skills. To pre-train the question embeddings, we propose to use product-based neural networks to recover the side information. As a result, adopting the pre-trained embeddings in existing deep KT models signiﬁcantly outperforms state-of-the-art baselines on three common KT datasets.",
            "source": "IJCAI[C]",
            "year": "2020",
            "paper_file_name": "2020-IJCAI-Improving Knowledge Tracing via Pre-training Question Embeddings-open.pdf"
        },
        {
            "model_name": "",
            "paper_title": "PTADisc: A Cross-Course Dataset Supporting Personalized Learning in Cold-Start Scenarios",
            "abstract": "The focus of our work is on diagnostic tasks in personalized learning, such as cognitive diagnosis and knowledge tracing. The goal of these tasks is to assess students' latent proficiency on knowledge concepts through analyzing their historical learning records. However, existing research has been limited to single-course scenarios; cross-course studies have not been explored due to a lack of dataset. We address this issue by constructing PTADisc , a Diverse, Immense, Student-centered dataset that emphasizes its sufficient Cross-course information for personalized learning. PTADisc includes 74 courses, 1, 530, 100 students, 4, 054 concepts, 225, 615 problems, and over 680 million student response logs. Based on PTADisc, we developed a model-agnostic Cross-Course Learner Modeling Framework (CCLMF) which utilizes relationships between students' proficiency across courses to alleviate the difficulty of diagnosing student knowledge state in cold-start scenarios. CCLMF uses a meta network to generate personalized mapping functions between courses. The experimental results on PTADisc verify the effectiveness of CCLMF with an average improvement of 4.2% on AUC. We also report the performance of baseline models for cognitive diagnosis and knowledge tracing over PTADisc, demonstrating that our dataset supports a wide scope of research in personalized learning. Additionally, PTADisc contains valuable programming logs and student-group information that are worth exploring in the future.",
            "source": "NeurIPS[C]",
            "year": "2023",
            "paper_file_name": "2023-NeurIPS-ptadisc-a-cross-course-dataset-supporting-personalized-learning-in-cold-start-scenarios-Paper-Datasets_and_Benchmarks.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing",
            "abstract": "Transformer has achieved excellent performance in the knowledge tracing (KT) task, but they are criticized for the manually selected input features for fusion and the defect of single global context modelling to directly capture students' forgetting behavior in KT, when the related records are distant from the current record in terms of time. To address the issues, this paper first considers adding convolution operations to the Transformer to enhance its local context modelling ability used for students' forgetting behavior, then proposes an evolutionary neural architecture search approach to automate the input feature selection and automatically determine where to apply which operation for achieving the balancing of the local/global context modelling. In the search space design, the original global path containing the attention module in Transformer is replaced with the sum of a global path and a local path that could contain different convolutions, and the selection of input features is also considered. To search the best architecture, we employ an effective evolutionary algorithm to explore the search space and also suggest a search space reduction strategy to accelerate the convergence of the algorithm. Experimental results on the two largest and most challenging education datasets demonstrate the effectiveness of the architecture found by the proposed approach.",
            "source": "NeurIPS[C]",
            "year": "2023",
            "paper_file_name": "2023-NeurIPS-Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Revisiting Knowledge Tracing: A Simple and Powerful Model",
            "abstract": "Advances in multimedia technology and its widespread application in education have made multimedia learning increasingly important. Knowledge Tracing (KT) is the key technology for achieving adaptive multimedia learning, aiming to monitor the degree of knowledge acquisition and predict students' performance during the learning process. Current KT research is dedicated to enhancing the performance of KT problems by integrating the most advanced deep learning techniques. However, this has led to increasingly complex models, which reduce model usability and divert researchers' attention away from exploring the core issues of KT. This paper aims to tackle the fundamental challenges of KT tasks, including the knowledge state representation and the core architecture design, and investigate a novel KT model that is both simple and powerful. We have revisited the KT task and propose the ReKT model. First, taking inspiration from the decision-making process of human teachers, we model the knowledge state of students from three distinct perspectives: questions, concepts, and domains. Second, building upon human cognitive development models, such as constructivism, we have designed a Forget-Response-Update (FRU) framework to serve as the core architecture for the KT task. The FRU is composed of just two linear regression units, making it an extremely lightweight framework. Extensive comparisons were conducted with 22 state-of-the-art KT models on 7 publicly available datasets. The experimental results demonstrate that ReKT outperforms all the comparative methods in question-based KT tasks, and consistently achieves the best (in most cases) or near-best performance in concept-based KT tasks. Furthermore, in comparison to other KT core architectures like Transformers or LSTMs, the FRU achieves superior prediction performance with approximately only 38% computing resources. Through an exploration of the ReKT model that is both simple and powerful, is able to offer new insights to future KT research. The code can be found at https://github.com/lilstrawberry/ReKT.",
            "source": "MM[C]",
            "year": "2024",
            "paper_file_name": "2024-MM-Revisiting Knowledge Tracing- A Simple and Powerful Model.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Rebalancing Discriminative Responses for Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) is a crucial task in computer-aided education and intelligent tutoring systems, predicting students' performance on new questions from their responses to prior ones. An accurate KT model can capture a student's mastery level of different knowledge topics, as reflected in their predicted performance on different questions. This helps improve the learning efficiency by suggesting appropriate new questions that complement students' knowledge states. However, current KT models have significant drawbacks that they neglect the imbalanced discrimination of historical responses. A significant proportion of question responses provide limited information for discerning students' knowledge mastery, such as those that demonstrate uniform performance across different students. Optimizing the prediction of these cases may increase overall KT accuracy, but also negatively impact the model's ability to trace personalized knowledge states, especially causing a deceptive surge of performance. Towards this end, we propose a framework to reweight the contribution of different responses based on their discrimination in training. Additionally, we introduce an adaptive predictive score fusion technique to maintain accuracy on less discriminative responses, achieving proper balance between student knowledge mastery and question difficulty. Experimental results demonstrate that our framework enhances the performance of three mainstream KT methods on three widely-used datasets.",
            "source": "TOIS[T]",
            "year": "2025",
            "paper_file_name": "2025-TOIS-Rebalancing Discriminative Responses for Knowledge Tracing.pdf"
        },
        {
            "model_name": "PST",
            "paper_title": "PST: Measuring Skill Proficiency in Programming Exercise Process via Programming Skill Tracing",
            "abstract": "Programming has become an important skill for individuals nowadays. For the demand to improve personal programming skill, tracking programming skill proficiency is getting more and more important. However, few researchers pay attention to measuring the programming skill of learners. Most of existing studies on learner capability portrait only made use of the exercise results, while the rich behavioral information contained in programming exercise process remains unused. Therefore, we propose a model that measures skill proficiency in programming exercise process named Programming Skill Tracing (PST). We designed Code Information Graph (CIG) to represent the feature of learners' solution code, and Code Tracing Graph (CTG) to measure the changes between the adjacent submissions. Furthermore, we divided programming skill into programming knowledge and coding ability to get more fine-grained assessment. Finally, we conducted various experiments to verify the effectiveness and interpretability of our PST model.",
            "source": "SIGIR[C]",
            "year": "2022",
            "paper_file_name": "2022-SIGIR-PST@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Forgetting-aware Linear Bias for Attentive Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) aims to track proficiency based on a question-solving history, allowing us to offer a streamlined curriculum. Recent studies actively utilize attention-based mechanisms to capture the correlation between questions and combine it with the learner's characteristics for responses. However, our empirical study shows that existing attention-based KT models neglect the learner's forgetting behavior, especially as the interaction history becomes longer. This problem arises from the bias that overprioritizes the correlation of questions while inadvertently ignoring the impact of forgetting behavior. This paper proposes a simple-yet-effective solution, namely Forgetting-aware Linear Bias (FoLiBi), to reflect forgetting behavior as a linear bias. Despite its simplicity, FoLiBi is readily equipped with existing attentive KT models by effectively decomposing question correlations with forgetting behavior. FoLiBi plugged with several KT models yields a consistent improvement of up to 2.58% in AUC over state-of-the-art KT models on four benchmark datasets.",
            "source": "CIKM[C]",
            "year": "2023",
            "paper_file_name": "2023-CIKM-Forgetting-aware Linear Bias for Attentive Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Knowledge Query Network for Knowledge Tracing: How Knowledge Interacts with Skills",
            "abstract": "Knowledge Tracing (KT) is to trace the knowledge of students as they\nsolve a sequence of problems represented by their related skills. This\ninvolves abstract concepts of students' states of knowledge and the\ninteractions between those states and skills. Therefore, a KT model\nis designed to predict whether students will give correct answers\nand to describe such abstract concepts. However, existing methods\neither give relatively low prediction accuracy or fail to explain\nthose concepts intuitively. In this paper, we propose a new model\ncalled Knowledge Query Network (KQN) to solve these problems.\nKQN uses neural networks to encode student learning activities\ninto knowledge state and skill vectors, and models the interactions\nbetween the two types of vectors with the dot product. Through\nthis, we introduce a novel concept called probabilistic skill similarity\nthat relates the pairwise cosine and Euclidean distances between\nskill vectors to the odds ratios of the corresponding skills, which\nmakes KQN interpretable and intuitive.\nOn four public datasets, we have carried out experiments to show\nthe following: 1. KQN outperforms all the existing KT models based\non prediction accuracy. 2. The interaction between the knowledge\nstate and skills can be visualized for interpretation. 3. Based on\nprobabilistic skill similarity, a skill domain can be analyzed with\nclustering using the distances between the skill vectors of KQN.\n4. For different values of the vector space dimensionality, KQN\nconsistently exhibits high prediction accuracy and a strong positive\ncorrelation between the distance matrices of the skill vectors.",
            "source": "LAK[C]",
            "year": "2019",
            "paper_file_name": "2019-LAK-Knowledge Query Network for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Uncertainty-aware Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) is crucial in education assessment,\nwhich focuses on depicting students’ learning states and as-\nsessing students’ mastery of subjects. With the rise of mod-\nern online learning platforms, particularly massive open on-\nline courses (MOOCs), an abundance of interaction data has\ngreatly advanced the development of the KT technology. Pre-\nvious research commonly adopts deterministic representation\nto capture students’ knowledge states, which neglects the un-\ncertainty during student interactions and thus fails to model\nthe true knowledge state in learning process. In light of this,\nwe propose an Uncertainty-Aware Knowledge Tracing model\n(UKT) which employs stochastic distribution embeddings\nto represent the uncertainty in student interactions, with a\nWasserstein self-attention mechanism designed to capture the\ntransition of state distribution in student learning behaviors.\nAdditionally, we introduce the aleatory uncertainty-aware\ncontrastive learning loss, which strengthens the model’s ro-\nbustness towards different types of uncertainties. Extensive\nexperiments on six real-world datasets demonstrate that UKT\nnot only significantly surpasses existing deep learning-based\nmodels in KT prediction, but also shows unique advantages\nin handling the uncertainty of student interactions.\nCode — https:\n//github.com/UncertaintyForKnowledgeTracing/UKT",
            "source": "AAAI[C]",
            "year": "2025",
            "paper_file_name": "2025-AAAI-Uncertainty-aware Knowledge Tracing.pdf"
        },
        {
            "model_name": "GKT",
            "paper_title": "Graph-based Knowledge Tracing: Modeling Student Proficiency Using Graph Neural Network",
            "abstract": "Recent advancements in computer-assisted learning systems have\ncaused an increase in the research of knowledge tracing, wherein stu-\ndent performance on coursework exercises is predicted over time.\nFrom the viewpoint of data structure, the coursework can be po-\ntentially structured as a graph. Incorporating this graph-structured\nnature into the knowledge tracing model as a relational inductive\nbias can improve its performance; however, previous methods, such\nas deep knowledge tracing, did not consider such a latent graph\nstructure. Inspired by the recent successes of the graph neural net-\nwork (GNN), we herein propose a GNN-based knowledge tracing\nmethod, i.e., graph-based knowledge tracing. Casting the knowl-\nedge structure as a graph enabled us to reformulate the knowledge\ntracing task as a time-series node-level classification problem in the\nGNN. As the knowledge graph structure is not explicitly provided\nin most cases, we propose various implementations of the graph\nstructure. Empirical validations on two open datasets indicated that\nour method could potentially improve the prediction of student\nperformance and demonstrated more interpretable predictions com-\npared to those of the previous methods, without the requirement\nof any additional information.",
            "source": "WI[C]",
            "year": "2019",
            "paper_file_name": "2019-WI-GKT@@.pdf"
        },
        {
            "model_name": "MF-DAKT",
            "paper_title": "Multi-Factors Aware Dual-Attentional Knowledge Tracing",
            "abstract": "With the increasing demands of personalized learning, knowledge tracing has become important which traces students' knowledge states based on their historical practices. Factor analysis methods mainly use two kinds of factors which are separately related to students and questions to model students' knowledge states. These methods use the total number of attempts of students to model students' learning progress and hardly highlight the impact of the most recent relevant practices. Besides, current factor analysis methods ignore rich information contained in questions. In this paper, we propose Multi-Factors Aware Dual-Attentional model (MF-DAKT) which enriches question representations and utilizes multiple factors to model students' learning progress based on a dual-attentional mechanism. More specifically, we propose a novel student-related factor which records the most recent attempts on relevant concepts of students to highlight the impact of recent exercises. To enrich questions representations, we use a pre-training method to incorporate two kinds of question information including questions' relation and difficulty level. We also add a regularization term about questions' difficulty level to restrict pre-trained question representations to fine-tuning during the process of predicting students' performance. Moreover, we apply a dual-attentional mechanism to differentiate contributions of factors and factor interactions to final prediction in different practice records. At last, we conduct experiments on several real-world datasets and results show that MF-DAKT can outperform existing knowledge tracing methods. We also conduct several studies to validate the effects of each component of MF-DAKT.",
            "source": "CIKM[C]",
            "year": "2021",
            "paper_file_name": "2021-CIKM-MF-DAKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Clickstream Knowledge Tracing: Modeling How Students Answer Interactive Online Questions",
            "abstract": "Knowledge tracing (KT) is a research topic which seeks to model the knowledge acquisition process of students by analyzing their past performance in answering questions, based on which their performance in answering future questions is predicted. However, existing KT models only consider whether a student answers a question correctly when the answer is submitted but not the in-question activities. We argue that the interaction involved in the in-question activities can at least partially reveal the thinking process of the student, and hopefully even the competence of acquiring or understanding each piece of the knowledge required for the question. Based on real student interaction clickstream data collected from an online learning platform on which students solve mathematics problems, we conduct clustering analysis for each question to show that clickstreams can reflect different student behaviors. We then propose the first clickstream-based KT model, dubbed clickstream knowledge tracing (CKT), which augments a basic KT model by modeling the clickstream activities of students when answering questions. We apply different variants of CKT and compare them with the baseline KT model which does not use clickstream data. Despite the limited number of questions with clickstream data and its noisy nature which may compromise the data quality, we show that incorporating clickstream data leads to performance improvement. Through this pilot study, we hope to open a new direction in KT research to analyze finer-grained interaction data of students on online learning platforms.",
            "source": "LAK[C]",
            "year": "2021",
            "paper_file_name": "2021-LAK-Clickstream Knowledge Tracing- Modeling How Students Answer Interactive Online Questions.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Cognitive Fluctuations Enhanced Attention Network for Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) involves using the historical records\nof student-learning interactions to anticipate their perfor-\nmance on forthcoming questions. Central to this process is the\nmodeling of human cognition to gain deeper insights into how\nknowledge is acquired and retained. Human cognition is char-\nacterized by two key features: long-term cognitive trends, re-\nflecting the gradual accumulation and stabilization of knowl-\nedge over time, and short-term cognitive fluctuations, which\narise from transient factors such as forgetting or momen-\ntary lapses in attention. Although existing attention-based KT\nmodels effectively capture long-term cognitive trends, they\noften fail to adequately address short-term cognitive fluctu-\nations. These limitations lead to overly smoothed cognitive\nfeatures and reduced model performance, especially when\nthe test data length exceeds the training data length. To ad-\ndress these problems, we propose FlucKT, a novel short-\nterm cognitive fluctuations enhanced attention network for\nKT tasks. FlucKT improves the attention mechanism in two\nways: First, by using a decomposition-based layer with causal\nconvolution to separate and dynamically reweight long-term\nand short-term cognitive features. Second, by introducing a\nkernelized bias attention score penalty to enhance focus on\nshort-term fluctuations, improving length generalization ca-\npabilities. Our contributions are validated through extensive\nexperiments on three real-world datasets, demonstrating sig-\nnificant improvements in length generalization and prediction\nperformance.",
            "source": "AAAI[C]",
            "year": "2025",
            "paper_file_name": "2025-AAAI-Cognitive Fluctuations Enhanced Attention Network for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Learning Logical Reasoning Using an Intelligent Tutoring System: A Hybrid Approach to Student Modeling",
            "abstract": "In our previous works, we presented Logic-Muse as an Intelligent Tutoring System that helps learners improve logical reasoning skills in multiple contexts. Logic-Muse components were validated and argued by experts throughout the designing process (ITS researchers, logicians, and reasoning psychologists). A catalog of reasoning errors (syntactic and semantic) has been established, in addition to an explicit representation of semantic knowledge and the structures and metastructures underlying conditional reasoning. A Bayesian network with expert validation has been developed and used in a Bayesian Knowledge Tracing (BKT) process that allows the inference of the learner skills. This paper presents an evaluation of the learner-model components in Logic-Muse (a bayesian learner model). We conducted a study and collected data from nearly 300 students who processed 48 reasoning activities. These data were used to develop a psychometric model for initializing the learner's model and validating the structure of the initial Bayesian network. We have also developed a neural architecture on which a model was trained to support a deep knowledge tracing (DKT) process. The proposed neural architecture improves the initial version of DKT by allowing the integration of expert knowledge (through the Bayesian Expert Validation Network) and allowing better generalization of knowledge with few samples. The results show a significant improvement in the predictive power of the learner model. The analysis of the results of the psychometric model also illustrates an excellent potential for improving the Bayesian network's structure and the learner model's initialization process.",
            "source": "AAAI[C]",
            "year": "2023",
            "paper_file_name": "2023-AAAI-Learning Logical Reasoning Using an Intelligent Tutoring System- A Hybrid Approach to Student Modeling Authors .pdf"
        },
        {
            "model_name": "",
            "paper_title": "Monitoring Student Progress for Learning Process-Consistent Knowledge Tracing",
            "abstract": "Abstract—Knowledge tracing (KT) is the task of tracing students' evolving knowledge state during learning, which has improved the learning efﬁciency. To facilitate KT's development, most existing methods pursue high accuracy of student performance prediction but neglect the consistency between students' dynamic knowledge state with their learning process. Moreover, they focus on learning outcomes at a single learning interaction, while student progress at continuous learning interactions is more instructive. In this paper, we explore a new paradigm for the KT task and propose a novel model named Learning Process-consistent Knowledge Tracing (LPKT), which captures the evolution of students' knowledge state through monitoring their learning progress. Speciﬁcally, we utilize both the positive effect of the learning gain and the negative effect of forgetting in learning to calculate student progress in continuous learning interactions. Then, considering that the rate of progress is student-speciﬁc, we extend LPKT to LPKT-S by explicitly distinguishing the individual progress rate of each student. Extensive experimental results on three public datasets demonstrate that LPKTand LPKT-S could obtain more appropriate knowledge states in line with the learning process. Moreover, LPKTand LPKT-S outperform state-of-the-art KTmethods on student performance prediction. Our work indicates a promising future research direction for KT, which is highly interpretable and accurate.",
            "source": "TKDE[T]",
            "year": "2023",
            "paper_file_name": "2023-TKDE-Monitoring Student Progress for Learning Process-Consistent Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Interpretable Knowledge Tracing with Multiscale State Representation",
            "abstract": "Knowledge Tracing (KT) is vital for education, continuously monitoring students' knowledge states (mastery of knowledge) as they interact with online education materials. Despite significant advancements in deep learning-based KT models, existing approaches often struggle to strike the right balance in granularity, leading to either overly coarse or excessively fine tracing and representation of students' knowledge states, thereby limiting their performance. Additionally, achieving a high-performing model while ensuring interpretability presents a challenge. Therefore, in this paper, we propose a novel approach called Multiscale-state-based Interpretable Knowledge Tracing (MIKT). Specifically, MIKT traces students' knowledge states on two scales: a coarse-grained representation to trace students' domain knowledge state, and a fine-grained representation to monitor their conceptual knowledge state. Furthermore, the classical psychological measurement model, IRT (Item Response Theory), is introduced to explain the prediction process of MIKT, enhancing its interpretability without sacrificing performance. Additionally, we extended the Rasch representation method to effectively handle scenarios where questions are associated with multiple concepts, making it more applicable to real-world situations. We extensively compared MIKT with 20 state-of-the-art KT models on four widely-used public datasets. Experimental results demonstrate that MIKT outperforms other models while maintaining its interpretability. Moreover, experimental observations have revealed that our proposed extended Rasch representation method not only benefits MIKT but also significantly improves the performance of other KT baseline models. The code can be found at https://github.com/lilstrawberry/MIKT.",
            "source": "WWW[C]",
            "year": "2024",
            "paper_file_name": "2024-WWW-Interpretable Knowledge Tracing with Multiscale State Representation.pdf"
        },
        {
            "model_name": "",
            "paper_title": "PYKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models",
            "abstract": "Knowledge tracing (KT) is the task of using students' historical learning interaction data to model their knowledge mastery over time so as to make predictions on their future interaction performance. Recently, remarkable progress has been made of using various deep learning techniques to solve the KT problem. However, the success behind deep learning based knowledge tracing (DLKT) approaches is still left somewhat unknown and proper measurement and analysis of these DLKT approaches remain a challenge. First, data preprocessing procedures in existing works are often private and custom, which limits experimental standardization. Furthermore, existing DLKT studies often differ in terms of the evaluation protocol and are far away real-world educational contexts. To address these problems, we introduce a comprehensive python based benchmark platform, PYKT, to guarantee valid comparisons across DLKT methods via thorough evaluations. The PYKT library consists of a standardized set of integrated data preprocessing procedures on 7 popular datasets across different domains, and 10 frequently compared DLKT model implementations for transparent experiments. Results from our fine-grained and rigorous empirical KT studies yield a set of observations and suggestions for effective DLKT, e.g., wrong evaluation setting may cause label leakage that generally leads to performance inflation; and the improvement of many DLKT approaches is minimal compared to the very first DLKT model proposed by Piech et al. [25]. We have open sourced PYKT and our experimental results at https://pykt.org/. We welcome contributions from other research groups and practitioners.",
            "source": "NeurIPS[C]",
            "year": "2022",
            "paper_file_name": "2022-NeurIPS-PYKT.pdf"
        },
        {
            "model_name": "",
            "paper_title": "A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models",
            "abstract": "Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in modeling the individual question information. This is crucial because students' knowledge acquisition on questions that share the same set of knowledge components (KCs) may vary significantly. However, due to the large question bank, the average number of interactions per question may not be sufficient. This limitation can potentially result in overfitting of the question embedding and inaccurate question knowledge acquisition state that relies on its corresponding question representation. Furthermore, there is a considerable portion of questions receiving relatively less interaction from students in comparison to the majority of questions. This can further increase the risk of overfitting and lower the accuracy of the obtained question knowledge acquisition state. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This makes teachers accept the rationale behind the prediction results and utilize them to design teaching activities and tailored learning strategies for students. However, the inherent black-box nature of deep learning techniques often poses a hurdle for teachers to fully embrace the model's prediction results. To address these challenges, we propose a Question-centric Multi-experts Contrastive Learning framework for KT called Q-MCKT. This framework explicitly models students' knowledge acquisition state at both the question and concept levels. It leverages the mixture of experts technique to capture a more robust and accurate knowledge acquisition state in both question and concept levels for prediction. Additionally, a fine-grained question-centric contrastive learning task is introduced to enhance the representations of less interactive questions and improve the accuracy of their corresponding question knowledge acquisition states. Moreover, Q-MCKT utilizes an item response theory-based prediction layer to generate interpretable prediction results based on the knowledge acquisition states obtained from the question and concept knowledge acquisition modules. We evaluate the proposed Q-MCKT framework on four public real-world educational datasets. The experimental results demonstrate that our approach outperforms a wide range of deep learning-based KT models in terms of prediction accuracy while maintaining better model interpretability. To ensure reproducibility, we have provided all the datasets and code on our website at https://github.com/rattlesnakey/Q-MCKT.",
            "source": "TKDD[T]",
            "year": "2025",
            "paper_file_name": "2025-TKDD-A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Incremental Knowledge Tracing from Multiple Schools",
            "abstract": "Knowledge tracing is the task of predicting a learner's future performance based on the history of the learner's performance. Current knowledge tracing models are built based on an extensive set of data that are collected from multiple schools. However, it is impossible to pool learner's data from all schools, due to data privacy and PDPA policies. Hence, this paper explores the feasibility of building knowledge tracing models while preserving the privacy of learners' data within their respective schools. This study is conducted using part of the ASSISTment 2009 dataset, with data from multiple schools being treated as separate tasks in a continual learning framework. The results show that learning sequentially with the Self Attentive Knowledge Tracing (SAKT) algorithm is able to achieve considerably similar performance to that of pooling all the data together.",
            "source": "AAAI[C]",
            "year": "2022",
            "paper_file_name": "2022-AAAI-Incremental Knowledge Tracing from Multiple Schools.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
            "abstract": "Making the content generated by Large Language Model (LLM),\naccurate, credible and traceable is crucial, especially in complex\nknowledge-intensive tasks that require multi-step reasoning and\neach step needs knowledge to solve. Retrieval-augmented genera-\ntion is good potential to solve this problem. However, where and\nhow to introduce Information Retrieval (IR) to LLM is a big chal-\nlenge. Previous work has the problems that wrong knowledge re-\ntrieved by IR misleads the LLM and interaction between IR and\nLLM breaks the reasoning chain of LLM. This paper proposes a\nnovel framework named Search-in-the-Chain (SearChain) for the\ninteraction between LLM and IR to solve the challenges. First, LLM\ngenerates the reasoning chain named Chain-of-Query (CoQ) where\neach node consists of an IR-oriented query-answer pair. Second,\nIR verifies the answer of each node of CoQ. It corrects the answer\nthat is not consistent with the retrieved information when IR gives\nhigh confidence, which improves the credibility. Third, LLM can\nindicate its missing knowledge in CoQ and rely on IR to provide\nthis knowledge to LLM. These operations improve the accuracy in\nterms of reasoning and knowledge. Finally, SearChain generates the\nreasoning process and marks references to supporting documents\nfor each reasoning step, which improves traceability. Interaction\nwith IR in SearChain forms a novel reasoning path based on a\ntree, which enables LLM to dynamically modify the direction of\nreasoning. Experiments show that SearChain outperforms state-of-\nthe-art baselines on complex knowledge-intensive tasks including\nmulti-hop Q&A, slot filling, fact checking, and long-form Q&A.",
            "source": "WWW[C]",
            "year": "2024",
            "paper_file_name": "2024-WWW-Search-in-the-Chain- Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks.pdf"
        },
        {
            "model_name": "SAINT+",
            "paper_title": "SAINT+: Integrating Temporal Features for EdNet Correctness Prediction",
            "abstract": "We propose SAINT+, a successor of SAINT which is a Transformer based knowledge tracing model that separately processes exercise information and student response information. Following the architecture of SAINT, SAINT+ has an encoder-decoder structure where the encoder applies self-attention layers to a stream of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to streams of response embeddings and encoder output. Moreover, SAINT+ incorporates two temporal feature embeddings into the response embeddings: elapsed time, the time taken for a student to answer, and lag time, the time interval between adjacent learning activities. We empirically evaluate the effectiveness of SAINT+ on EdNet, the largest publicly available benchmark dataset in the education domain. Experimental results show that SAINT+ achieves state-of-the-art performance in knowledge tracing with an improvement of 1.25% in area under receiver operating characteristic curve compared to SAINT, the current state-of-the-art model in EdNet dataset.",
            "source": "LAK[C]",
            "year": "2021",
            "paper_file_name": "2021-LAK-SAINT+@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Open-Ended Knowledge Tracing for Computer Science Education",
            "abstract": "In education applications, knowledge tracing\nrefers to the problem of estimating students'\ntime-varying concept/skill mastery level from\ntheir past responses to questions and predicting\ntheir future performance. One key limitation\nof most existing knowledge tracing methods is\nthat they treat student responses to questions\nas binary-valued, i.e., whether they are cor-\nrect or incorrect. Response correctness analy-\nsis/prediction ignores important information on\nstudent knowledge contained in the exact con-\ntent of the responses, especially for open-ended\nquestions. In this paper, we conduct the first\nexploration into open-ended knowledge tracing\n(OKT) by studying the new task of predicting\nstudents' exact open-ended responses to ques-\ntions. Our work is grounded in the domain of\ncomputer science education with programming\nquestions. We develop an initial solution to\nthe OKT problem, a student knowledge-guided\ncode generation approach, that combines pro-\ngram synthesis methods using language models\nwith student knowledge tracing methods. We\nalso conduct a series of quantitative and quali-\ntative experiments on a real-world student code\ndataset to validate OKT and demonstrate its\npromise in educational applications.",
            "source": "EMNLP[C]",
            "year": "2022",
            "paper_file_name": "2022-EMNLP-Open-Ended Knowledge Tracing for Computer Science Education.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Interactive Code Information Integrated Programming Knowledge Tracing",
            "abstract": "Programming Knowledge Tracing (PKT) aims to estimate students' programming proficiency by analyzing their historical coding activities. It is important to leverage interactive information for PKT code representations, as code submissions are problem-specific interactive solutions evaluated by Online Judge (OJ) feedback. However, most existing PKT models only rely on pre-trained language models to capture static code information, overlooking feedback scores and problem-specific context. To address this issue, we propose an Interactive Information integrated Code Embedding for Programming Knowledge Tracing (IICE-PKT). This model learns a unified code representation by integrating comprehensive interactive information, including code text, OJ feedback scores, problem statement text, problem-skill correlations, and problem difficulties. IICE-PKT consists of three modules: Problem Representation module generates enhanced problem embeddings based on problem-skill correlations and problem difficulties; Code Representation module combines fine-tuned CodeBERT-based code text embeddings, GPT-generated problem text embeddings, and enhanced problem embeddings with supervised feedback scores to produce a unified code representation; Dual-Sequence Modeling module employs two independent GRUs to separately model the problem sequence and the code sequence. Extensive experiments demonstrate the superiority of IICE-PKT. Ablation studies confirm that integrating interactive information into code representations significantly enhances effectiveness.",
            "source": "SIGIR[C]",
            "year": "2025",
            "paper_file_name": "2025-SIGIR-Interactive Code Information Integrated Programming Knowledge Tracing.pdf"
        },
        {
            "model_name": "ATDKT",
            "paper_title": "Enhancing Deep Knowledge Tracing with Auxiliary Tasks",
            "abstract": "Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interactions with intelligent tutoring systems. Recent studies have applied multiple types of deep neural networks to solve the KT problem. However, there are two important factors in real-world educational data that are not well represented. First, most existing works augment input representations with the co-occurrence matrix of questions and knowledge components1 (KCs) but fail to explicitly integrate such intrinsic relations into the fnal response prediction task. Second, the individualized historical performance of students has not been well captured. In this paper, we proposed AT-DKT to improve the prediction performance of the original deep knowledge tracing model with two auxiliary learning tasks, i.e., question tagging (QT) prediction task and individualized prior knowledge (IK) prediction task. Specifcally, the QT task helps learn better question representations by predicting whether questions contain specifc KCs. The IK task captures students' global historical performance by progressively predicting student-level prior knowledge that is hidden in students' historical learning interactions. We conduct comprehensive experiments on three real-world educational datasets and compare the proposed approach to both deep sequential KT models and non-sequential models. Experimental results show that AT-DKT outperforms all sequential models with more than 0.9% improvements of AUC for all datasets, and is almost the second best compared to non-sequential models. Furthermore, we conduct both ablation studies and quantitative analysis to show the efectiveness of auxiliary tasks and the superior prediction outcomes of AT-DKT. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt­team/pykt-toolkit 2.",
            "source": "WWW[C]",
            "year": "2023",
            "paper_file_name": "2023-WWW-ATDKT@@Enhancing Deep Knowledge Tracing with Auxiliary Tasks.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Equity and Fairness of Bayesian Knowledge Tracing",
            "abstract": "We consider the equity and fairness of curricula derived from\nKnowledge Tracing models. We begin by defining a unify-\ning notion of an equitable tutoring system as a system that\nachieves maximum possible knowledge in minimal time for\neach student interacting with it. Realizing perfect equity re-\nquires tutoring systems that can provide individualized cur-\ricula per student. In particular, we investigate the design of\nequitable tutoring systems that derive their curricula from\nKnowledge Tracing models.\nWe first show that the clas-\nsical Bayesian Knowledge Tracing (BKT) model and their\nderived curricula can fall short of achieving equitable tutor-\ning. To overcome this issue, we then propose a novel model,\nBayesian-Bayesian Knowledge Tracing (B2KT), that natu-\nrally allows online individualization. We demonstrate that\ncurricula derived from our model are more effective and equi-\ntable than those derived from existing models. Furthermore,\nwe highlight that improving models with a focus on the fair-\nness of next-step predictions can be insufficient to develop\nequitable tutoring systems.",
            "source": "EDM[C]",
            "year": "2022",
            "paper_file_name": "2022-EDM-Bayesian-Bayesian Knowledge Tracing.pdf"
        },
        {
            "model_name": "RKT",
            "paper_title": "RKT : Relation-Aware Self-Attention for Knowledge Tracing",
            "abstract": "The world has transitioned into a new phase of online learning in\nresponse to the recent Covid19 pandemic. Now more than ever,\nit has become paramount to push the limits of online learning in\nevery manner to keep flourishing the education system. One crucial\ncomponent of online learning is Knowledge Tracing (KT). The aim\nof KT is to model student's knowledge level based on their answers\nto a sequence of exercises referred as interactions. Students ac-\nquire their skills while solving exercises and each such interaction\nhas a distinct impact on student ability to solve a future exercise.\nThis impact is characterized by 1) the relation between exercises\ninvolved in the interactions and 2) student forget behavior. Tradi-\ntional studies on knowledge tracing do not explicitly model both\nthe components jointly to estimate the impact of these interactions.\nIn this paper, we propose a novel Relation-aware self-attention\nmodel for Knowledge Tracing (RKT). We introduce a relation-aware\nself-attention layer that incorporates the contextual information.\nThis contextual information integrates both the exercise relation in-\nformation through their textual content as well as student perform-\nance data and the forget behavior information through modeling\nan exponentially decaying kernel function. Extensive experiments\non three real-world datasets, among which two new collections are\nreleased to the public, show that our model outperforms state-of-\nthe-art knowledge tracing methods. Furthermore, the interpretable\nattention weights help visualize the relation between interactions\nand temporal patterns in the human learning process.",
            "source": "CIKM[C]",
            "year": "2020",
            "paper_file_name": "2020-CIKM-RKT@@Relation-Aware Self-Attention for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Mitigating Biases in Student Performance Prediction via Attention-Based Personalized Federated Learning",
            "abstract": "Traditional learning-based approaches to student modeling generalize poorly to underrepresented student groups due to biases in data availability. In this paper, we propose a methodology for predicting student performance from their online learning activities that optimizes inference accuracy over different demographic groups such as race and gender. Building upon recent foundations in federated learning, in our approach, personalized models for individual student subgroups are derived from a global model aggregated across all student models via meta-gradient updates that account for subgroup heterogeneity. To learn better representations of student activity, we augment our approach with a self-supervised behavioral pretraining methodology that leverages multiple modalities of student behavior (e.g., visits to lecture videos and participation on forums), and include a neural network attention mechanism in the model aggregation stage. Through experiments on three real-world datasets from online courses, we demonstrate that our approach obtains substantial improvements over existing student modeling baselines in predicting student learning outcomes for all subgroups. Visual analysis of the resulting student embeddings confirm that our personalization methodology indeed identifies different activity patterns within different subgroups, consistent with its stronger inference ability compared with the baselines.",
            "source": "CIKM[C]",
            "year": "2022",
            "paper_file_name": "2022-CIKM-Mitigating Biases in Student Performance Prediction via Attention-Based Personalized Federated Learning-close.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Mitigating Cold-Start Problems in Knowledge Tracing with Large Language Models: An Attribute-aware Approach",
            "abstract": "Knowledge Tracing (KT) is a crucial research task for dynamically monitoring students' knowledge states, particularly in online education systems. Recently, knowledge tracing has gained significant attention and in-depth research. Most existing methods rely on students' response data for question understanding and modeling, which helps better updating students' knowledge states. Meanwhile, question ID is utilized to indicate and represent questions. However, this presents a challenge when transitioning to new, cold-start questions that few students has answered before. Also, prior work has overlooked the semantic modeling of questions, which could better assist in modeling the transfer of students' knowledge states. In this paper, we explore leveraging the power of Large Language Models (LLMs) to help understand questions for knowledge tracing, which benefits mitigating cold-start and sparse problems and modeling the transfer of students' knowledge states in a sophisticated manner. Specifically, we first design an attribute estimation module to estimate the attribute of the questions (e.g., difficulty, ability requirements, expected response time) by prompting Large Language Models. Subsequently, we have developed a question embedding module that incorporates graph attention network to effectively utilizing these attributes. Extensive experiments on various datasets demonstrate that our model outperforms existing state-of-the-art models and effectively addresses the problems of cold-start and sparsity. In addition, due to the estimation of multiple attributes of the questions, our model exhibits superior interpretability.",
            "source": "CIKM[C]",
            "year": "2024",
            "paper_file_name": "2024-CIKM-Mitigating Cold-Start Problems in Knowledge Tracing with Large Language Models- An Attribute-aware Approach.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Enhancing Programming Knowledge Tracing by Interacting Programming Skills and Student Code",
            "abstract": "Programming education has received extensive attention in recent years due to the increasing demand for programming ability in almost all industries. Educational institutions have widely employed online judges for programming training, which can help teachers automatically assess programming assignments by executing students' code with test cases. However, a more important teaching process with online judges should be to evaluate how students master each of the programming skills such as strings or pointers, so that teachers may give personalized feedback and help them proceed to the success more efficiently. Previous studies have adopted deep models of knowledge tracing to evaluate a student's mastery level of skills during the interaction with programming exercises. However, existing models generally follow the conventional assumption of knowledge tracing that each programming exercise requires only one skill, whereas in practice a programming exercise usually inspects the comprehensive use of multiple skills. Moreover, the feature of student code is often simply concatenated with other input features without the consideration of its relationship with the inspected programming skills. To bridge the gap, we propose a simple attention-based approach to learn from student code the features reflecting the multiple programming skills inspected by each programming exercise. In particular, we first use a program embedding method to obtain the representations of student code. Then we use the skill embeddings of each programming exercise to query the embeddings of student code and form an aggregated hidden state representing how the inspected skills are used in the student code. We combine the learned hidden state with DKT (Deep Knowledge Tracing), an LSTM (Long Short-Term Memory)-based knowledge tracing model, and show the improvements over baseline model. We point out some possible directions to improve the current work.",
            "source": "LAK[C]",
            "year": "2022",
            "paper_file_name": "2022-LAK-Enhancing Programming Knowledge Tracing by Interacting Programming Skills and Student Code-open.pdf"
        },
        {
            "model_name": "SAINT",
            "paper_title": "Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing",
            "abstract": "In this paper, we propose a novel Transformer-based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT has an encoder-decoder structure where the exercise and response embedding sequences separately enter, respectively, the encoder and the decoder. The encoder applies self-attention layers to the sequence of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to the sequence of response embeddings. This separation of input allows us to stack attention layers multiple times, resulting in an improvement in area under receiver operating characteristic curve (AUC). To the best of our knowledge, this is the first work to suggest an encoder-decoder model for knowledge tracing that applies deep self-attentive layers to exercises and responses separately. We empirically evaluate SAINT on a large-scale knowledge tracing dataset, EdNet, collected by an active mobile education application, Santa, which has 627,347 users, 72,907,005 response data points as well as a set of 16,175 exercises gathered since 2016. The results show that SAINT achieves state-of-the-art performance in knowledge tracing with an improvement of 1.8% in AUC compared to the current state-of-the-art model.",
            "source": "L@S[C]",
            "year": "2020",
            "paper_file_name": "2020-L@S-SAINT@@.pdf"
        },
        {
            "model_name": "DGEKT",
            "paper_title": "DGEKT: A Dual Graph Ensemble Learning Method for Knowledge Tracing",
            "abstract": "Knowledge tracing aims to trace students' evolving knowledge states by predicting their future performance on concept-related exercises. Recently, some graph-based models have been developed to incorporate the relationships between exercises to improve knowledge tracing, but only a single type of relationship information is generally explored. In this article, we present a novel Dual Graph Ensemble learning method for Knowledge Tracing (DGEKT), which establishes a dual graph structure of students' learning interactions to capture the heterogeneous exercise–concept associations and interaction transitions by hypergraph modeling and directed graph modeling, respectively. To combine the dual graph models, we introduce the technique of online knowledge distillation. This choice arises from the observation that, while the knowledge tracing model is designed to predict students' responses to the exercises related to different concepts, it is optimized merely with respect to the prediction accuracy on a single exercise at each step. With online knowledge distillation, the dual graph models are adaptively combined to form a stronger ensemble teacher model, which provides its predictions on all exercises as extra supervision for better modeling ability. In the experiments, we compare DGEKT against eight knowledge tracing baselines on three benchmark datasets, and the results demonstrate that DGEKT achieves state-of-the-art performance.",
            "source": "TOIS[T]",
            "year": "2024",
            "paper_file_name": "2024-TOIS-DGEKT@@A Dual Graph Ensemble Learning Method for Knowledge Tracing.pdf"
        },
        {
            "model_name": "codeDKT",
            "paper_title": "Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks",
            "abstract": "Knowledge tracing (KT) models are a popular approach for predicting students' future performance at practice problems using their prior attempts. Though many innovations have been made in KT, most models including the state-of-the-art Deep KT (DKT) mainly leverage each student's response either as correct or incorrect, ignoring its content. In this work, we propose Code-based Deep Knowledge Tracing (Code-DKT), a model that uses an attention mechanism to automatically extract and select domain-specific code features to extend DKT. We compared the effectiveness of Code-DKT against Bayesian and Deep Knowledge Tracing (BKT and DKT) on a dataset from a class of 50 students attempting to solve 5 introductory programming assignments. Our results show that Code-DKT consistently outperforms DKT by 3.07 −4.00% AUC across the 5 assignments, a comparable improvement to other state-of-the-art domain-general KT models over DKT. Finally, we analyze problem-specific performance through a set of case studies for one assignment to demonstrate when and how code features improve Code-DKT's predictions.",
            "source": "EDM[C]",
            "year": "2022",
            "paper_file_name": "2022-EDM-codeDKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs",
            "abstract": "Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components/skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLM's accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work.",
            "source": "LAK[C]",
            "year": "2025",
            "paper_file_name": "2025-LAK-Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs.pdf"
        },
        {
            "model_name": "DKVMN",
            "paper_title": "Dynamic Key-Value Memory Networks for Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.",
            "source": "WWW[C]",
            "year": "2017",
            "paper_file_name": "2017-WWW-DKVMN@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Cognition-Mode Aware Variational Representation Learning Framework for Knowledge Tracing",
            "abstract": "Abstract—The Knowledge Tracing (KT) task plays a crucial role in personalized learning, and its purpose is to predict student responses based on their historical practice behavior sequence. However, the KT task suffers from data sparsity, which makes it challenging to learn robust representations for students with few practice records and increases the risk of model overfitting. Therefore, in this paper, we propose a Cognition-Mode Aware Variational Representation Learning Framework (CMVF) that can be directly applied to existing KT methods. Our framework uses a probabilistic model to generate a distribution for each student, accounting for uncertainty in those with limited practice records, and estimate the student's distribution via variational inference (VI). In addition, we also introduce a cognition-mode aware multinomial distribution as prior knowledge that constrains the posterior student distributions learning, so as to ensure that students with similar cognition modes have similar distributions, avoiding overwhelming personalization for students with few practice records. At last, extensive experimental results confirm that CMVF can effectively aid existing KT methods in learning more robust student representations. Our code is available at https://github.com/zmy-9/CMVF.",
            "source": "ICDM[C]",
            "year": "2023",
            "paper_file_name": "2023-ICDM-Cognition-Mode Aware Variational Representation Learning Framework for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Multi-Scaled Attentive Knowledge Tracing",
            "abstract": "Abstract—Learning is a dynamic, complex, and time-series\nprocess. Knowledge tracing (KT) aims to simulate learners’\nlearning process by using learners’ behavioral performance in\npast learning activities. In recent years, self-attentive mechanisms\nhave been widely used in KT model. The literature shows that\nattention-based KT models generally outperform traditional deep\nknowledge tracing models. In order to simulate the learning\nprocess of learners more effectively we propose a new multi-\nscale attentive knowledge tracing model for KT. Speciﬁcally, the\nmodel uses multi-scale multi-head attention to capture learner\nfeatures at different time scales and use them to model learners’\nlearning behaviors. We also use relative position encoding to\nmaintain the consistency of location information across multiple\nscales of attention. Experiments on real datasets show that our\nmodel outperforms state-of-the-art KT methods.",
            "source": "WAIE[C]",
            "year": "2022",
            "paper_file_name": "2022-WAIE-Multi-Scaled Attentive Knowledge Tracing-close.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Fine-Grained Interaction Modeling with Multi-Relational Transformer for Knowledge Tracing",
            "abstract": "Knowledge tracing, the goal of which is predicting students' future performance given their past question response sequences to trace their knowledge states, is pivotal for computer-aided education and intelligent tutoring systems. Although many technical efforts have been devoted to modeling students based on their question-response sequences, fine-grained interaction modeling between question-response pairs within each sequence is underexplored. This causes question-response representations less contextualized and further limits student modeling. To address this issue, we first conduct a data analysis and reveal the existence of complex cross effects between different question-response pairs within a sequence. Consequently, we propose MRT-KT, a multi-relational transformer for knowledge tracing, to enable fine-grained interaction modeling between question-response pairs. It introduces a novel relation encoding scheme based on knowledge concepts and student performance. Comprehensive experimental results show that MRT-KT outperforms state-of-the-art knowledge tracing methods on four widely-used datasets, validating the effectiveness of considering fine-grained interaction for knowledge tracing.",
            "source": "TOIS[T]",
            "year": "2023",
            "paper_file_name": "2023-TOIS-Fine-Grained Interaction Modeling with Multi-Relational Transformer for Knowledge Tracing.pdf"
        },
        {
            "model_name": "Deep-IRT",
            "paper_title": "Deep-IRT: Make Deep Learning Based Knowledge Tracing Explainable Using Item Response Theory",
            "abstract": "Deep learning based knowledge tracing model has been shown to outperform traditional knowledge tracing model without the need for human-engineered features, yet its parameters and representations have long been criticized for not being explainable. In this paper, we propose Deep-IRT which is a synthesis of the item response theory (IRT) model and a knowledge tracing model that is based on the deep neural network architecture called dynamic key-value memory network (DKVMN) to make deep learning based knowledge tracing explainable. Speciﬁcally, we use the DKVMN model to process the student's learning trajectory and estimate the item diﬃculty level and the student ability over time. Then, we use the IRT model to estimate the probability that a student will answer an item correctly using the estimated student ability and the item diﬃculty. Experiments show that the Deep-IRT model retains the performance of the DKVMN model, while it provides a direct psychological interpretation of both students and items.",
            "source": "EDM[C]",
            "year": "2019",
            "paper_file_name": "2019-EDM-Deep-IRT@@Make Deep Learning Based Knowledge Tracing Explainable Using Item Response Theory.pdf"
        },
        {
            "model_name": "",
            "paper_title": "How Does Bayesian Knowledge Tracing Model Emergence of Knowledge about a Mechanical System?",
            "abstract": "An interactive learning task was designed in a game format to help high school students acquire knowledge about a simple mechanical system involving a car moving on a ramp. This ramp game consisted of five challenges that addressed individual knowledge components with increasing difficulty. In order to investigate patterns of knowledge emergence during the ramp game, we applied the Monte Carlo Bayesian Knowledge Tracing (BKT) algorithm to 447 game segments produced by 64 student groups in two physics teachers' classrooms. Results indicate that, in the ramp game context, (1) the initial knowledge and guessing parameters were significantly highly correlated, (2) the slip parameter was interpretable monotonically, (3) low guessing parameter values were associated with knowledge emergence while high guessing parameter values were associated with knowledge maintenance, and (4) the transition parameter showed the speed of knowledge emergence. By applying the k-means clustering to ramp game segments represented in the three dimensional space defined by guessing, slip, and transition parameters, we identified seven clusters of knowledge emergence. We characterize these clusters and discuss implications for future research as well as for instructional game design.",
            "source": "LAK[C]",
            "year": "2015",
            "paper_file_name": "2015-LAK-How Does Bayesian Knowledge Tracing Model Emergence of Knowledge about a Mechanical System? .pdf"
        },
        {
            "model_name": "",
            "paper_title": "Leveraging Pedagogical Theories to Understand Student Learning Process with Graph-based Reasonable Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) is a crucial task in intelligent education,\nfocusing on predicting students' performance on given questions to\ntrace their evolving knowledge. The advancement of deep learning\nin this field has led to deep-learning knowledge tracing (DLKT)\nmodels that prioritize high predictive accuracy. However, many\nexisting DLKT methods overlook the fundamental goal of track-\ning students' dynamical knowledge mastery. These models do not\nexplicitly model knowledge mastery tracing processes or yield un-\nreasonable results that educators find difficulty to comprehend and\napply in real teaching scenarios. In response, our research conducts\na preliminary analysis of mainstream KT approaches to highlight\nand explain such unreasonableness. We introduce GRKT, a graph-\nbased reasonable knowledge tracing method to address these issues.\nBy leveraging graph neural networks, our approach delves into the\nmutual influences of knowledge concepts, offering a more accurate\nrepresentation of how the knowledge mastery evolves throughout\nthe learning process. Additionally, we propose a fine-grained and\npsychological three-stage modeling process as knowledge retrieval,\nmemory strengthening, and knowledge learning/forgetting, to con-\nduct a more reasonable knowledge tracing process. Comprehensive\nexperiments demonstrate that GRKT outperforms eleven baselines\nacross three datasets, not only enhancing predictive accuracy but\nalso generating more reasonable knowledge tracing results. This\nmakes our model a promising advancement for practical imple-\nmentation in educational settings. The source code is available at\nhttps://github.com/JJCui96/GRKT.",
            "source": "SIGKDD[C]",
            "year": "2024",
            "paper_file_name": "2024-SIGKDD-Leveraging Pedagogical Theories to Understand Student Learning Process with Graph-based Reasonable Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Continuous Personalized Knowledge Tracing: Modeling Long-Term Learning in Online Environments",
            "abstract": "With the advance of online education systems, accessibility to learning materials has increased. In these systems, students can practice independently and learn from different learning materials over long periods of time. As a result, it is essential to trace students' knowledge states over long learning sequences while maintaining a personalized model of each individual student's progress. However, the existing deep learning-based knowledge tracing models are either not personalized or not tailored for handling long sequences. Handling long sequences are especially essential in the online education environments, in where models are preferred to be updated with the newly collected user data in a timely manner as students could acquire knowledge on each learning activity. In this paper, we propose a knowledge tracing model, Continuous Personalized Knowledge Tracing (CPKT), that can mimic the real-world long-term continuous learning scenario by incorporating a novel online model training paradigm that is suitable for the knowledge tracing problem. To achieve personalized knowledge tracing, we propose two model components: 1) personalized memory slots to maintain learner's knowledge in a lifelong manner, and 2) personalized user embeddings that help to accurately predict the individual responses, correctly detect the personalized knowledge acquisition and forgetting patterns, and better interpret and analyze the learner's progress. Additionally, we propose transition-aware stochastic shared embedding according to the learning transition matrix to regularize the online model training. Extensive experiments on four real-world datasets showcase the effectiveness and superiority of CPKT, especially for students with longer sequences.",
            "source": "CIKM[C]",
            "year": "2023",
            "paper_file_name": "2023-CIKM-Continuous Personalized Knowledge Tracing- Modeling Long-Term Learning in Online Environments.pdf"
        },
        {
            "model_name": "DKT-forget",
            "paper_title": "Augmenting Knowledge Tracing by Considering Forgetting Behavior",
            "abstract": "Computer-aided education systems are now seeking to provide each\nstudent with personalized materials based on a student's individual\nknowledge. To provide suitable learning materials, tracing each\nstudent's knowledge over a period of time is important. However,\npredicting each student's knowledge is difficult because students\ntend to forget. The forgetting behavior is mainly because of two\nreasons: the lag time from the previous interaction, and the number\nof past trials on a question. Although there are a few studies that\nconsider forgetting while modeling a student's knowledge, some\nmodels consider only partial information about forgetting, whereas\nothers consider multiple features about forgetting, ignoring a stu-\ndent's learning sequence. In this paper, we focus on modeling and\npredicting a student's knowledge by considering their forgetting\nbehavior. We extend the deep knowledge tracing model [17], which\nis a state-of-the-art sequential model for knowledge tracing, to\nconsider forgetting by incorporating multiple types of information\nrelated to forgetting. Experiments on knowledge tracing datasets\nshow that our proposed model improves the predictive performance\nas compared to baselines. Moreover, we also examine that the com-\nbination of multiple types of information that affect the behavior\nof forgetting results in performance improvement.",
            "source": "WWW[C]",
            "year": "2019",
            "paper_file_name": "2019-WWW-DKT-forget@@.pdf"
        },
        {
            "model_name": "HawkesKT",
            "paper_title": "Temporal Cross-Effects in Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) aims to model students' knowledge level\nbased on their historical performance, which plays an important\nrole in computer-assisted education and adaptive learning. Recent\nstudies try to take temporal effects of past interactions into consid-\neration, such as the forgetting behavior. However, existing work\nmainly relies on time-related features or a global decay function to\nmodel the time-sensitive effects. Fine-grained temporal dynamics of\ndifferent cross-skill impacts have not been well studied (named as\ntemporal cross-effects). For example, cross-effects on some difficult\nskills may drop quickly, and the effects caused by distinct previous\ninteractions may also have different temporal evolutions, which\ncannot be captured in a global way.\nIn this work, we investigate fine-grained temporal cross-effects\nbetween different skills in KT. We first validate the existence of tem-\nporal cross-effects in real-world datasets through empirical studies.\nThen, a novel model, HawkesKT, is proposed to explicitly model the\ntemporal cross-effects inspired by the point process, where each\nprevious interaction will have different time-sensitive impacts on\nthe mastery of the target skill. HawkesKT adopts two components\nto model temporal cross-effects: 1) mutual excitation represents\nthe degree of cross-effects and 2) kernel function controls the\nadaptive temporal evolution. To the best of our knowledge, we\nare the first to introduce Hawkes process to model temporal cross-\neffects in KT. Extensive experiments on three benchmark datasets\nshow that HawkesKT is superior to state-of-the-art KT methods.\nRemarkably, our method also exhibits excellent interpretability and\nshows significant advantages in training efficiency, which makes it\nmore applicable in real-world large-scale educational settings.",
            "source": "WSDM[C]",
            "year": "2021",
            "paper_file_name": "2021-WSDM-HawkesKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Enhancing Knowledge Tracing Efficacy with Expert-defined Graphs: A Case Study in Introductory Physics Classes",
            "abstract": "Knowledge Tracing (KT) is essential in online education for tracking student progress and forecasting future performance. Despite the effectiveness of existing KT models, enhancing their educational interpretability and reliability remains crucial for both academic and practical applications. This study introduces an improved Graph-based Knowledge Tracing (GKT) model, enriched with domain expertise, instructional insights, and contextual features, to overcome current limitations. Our enhanced GKT model employs an expert-defined graph structure for more accurate domain knowledge representation. It integrates critical contextual features, like question difficulty and prompt usage, into the response matrix for a comprehensive context. Additionally, the model leverages second-order neighborhood features to more effectively capture complex interrelations between knowledge concepts. Validation using an Introductory Physics assignment dataset demonstrated that our updated GKT model surpasses its predecessor in both Area Under the Curve (AUC) and accuracy (ACC) metrics. These improvements are instrumental in refining knowledge graphs and developing personalized teaching strategies, thereby facilitating more effective and personalized educational experiences.",
            "source": "L@S[C]",
            "year": "2024",
            "paper_file_name": "2024-L@S-Enhancing Knowledge Tracing Efficacy with Expert-defined Graphs- A Case Study in Introductory Physics Classes.pdf"
        },
        {
            "model_name": "IEKT",
            "paper_title": "Tracing Knowledge State with Individual Cognition and Acquisition Estimation",
            "abstract": "Knowledge tracing, which dynamically estimates students' learning states by predicting their performance on answering questions, is an essential task in online education. One typical solution for knowledge tracing is based on Recurrent Neural Networks (RNNs), which represent students' knowledge states with the hidden states of RNNs. Such type of methods normally assumes that students have the same cognition level and knowledge acquisition sensitivity on the same question. Thus, they (i) predict students' responses by referring to their knowledge states and question representations, and (ii) update the knowledge states according to the question representations and students' responses. No explicit cognition level or knowledge acquisition sensitivity is considered in the above two processes. However, in real-world scenarios, students have different understandings on a question and have various knowledge acquisition after they finish the same question. In this paper, we propose a novel model called Individual Estimation Knowledge Tracing (IEKT), which estimates the students' cognition on the question before response prediction and assesses their knowledge acquisition sensitivity on the questions before updating the knowledge state. In the experiments, we compare IEKT with 11 knowledge tracing baselines on four benchmark datasets, and the results show IEKT achieves the state-of-the-art performance.",
            "source": "SIGIR[C]",
            "year": "2021",
            "paper_file_name": "2021-SIGIR-IEKT@@.pdf"
        },
        {
            "model_name": "XKT",
            "paper_title": "XKT: Toward Explainable Knowledge Tracing Model With Cognitive Learning Theories for Questions of Multiple Knowledge Concepts",
            "abstract": "Deep learning (DL) based knowledge tracing (KT) models have challenges for uninterpretable prediction and parameter representation in educational applications, though they achieved remarkable outcomes in predicting the exercise performance of students. This paper proposes a novel knowledge tracing model of high precision and interpretability (named XKT) for questions with multiple knowledge concepts based on cognitive learning theories and multidimensional item response theory (MIRT). The XKT consists of three differentiable network components: multi-featureembedding,cognitionprocessingnetwork,andMIRT-based neural predictor, which aim to provide an explainable prediction of student exercise performance. Speciﬁcally, in XKT, multi-feature embedding learns the rich semantic representation (e.g., knowledge distribution information) to enhance knowledge tracing using a cognition processing network. The cognition processing network performs selective perception, ability memory processing, and long-term knowledge memory processing to ensure the explainable factor representation for the MIRT-based neural predictor. Lastly, the MIRT-based neural predictor employs psychometric parameters to interpret student exercise predictions better. Extensive experiments on four real-world datasets show that XKT outperforms existing KT methods in predicting future learner responses. Moreover, ablation studies further show that XKT offers good interpretability of student performance predictions with multiple knowledge concepts, indicating excellent potential in real-world educational applications.",
            "source": "TKDE[T]",
            "year": "2024",
            "paper_file_name": "2024-TKDE-XKT@@Toward Explainable Knowledge Tracing Model With Cognitive Learning Theories for Questions of Multiple Knowledge Concepts.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Remembering is Not Applying: Interpretable Knowledge Tracing for Problem-solving Processes",
            "abstract": "Knowledge Tracing (KT) is a critical service in distance education, predicting students' future performance based on their responses to learning resources. The reasonable assessment of the knowledge state, along with accurate response prediction, is crucial for KT. However, existing KT methods prioritize fitting results and overlook attention to the problem-solving process. They equate the knowledge students memorize before problem-solving with the knowledge that can be acquired or applied during problem-solving, leading to dramatic fluctuations in knowledge states between mastery and non-mastery, with low interpretability. This paper explores knowledge transformation in problem-solving and proposes an interpretable model, Problem-solving Knowledge Tracing (PSKT). Specifically, we first present a knowledge-centered problem representation that enhances its expression by adjusting problem variability. Then, we meticulously designed a Sequential Neural Network (SNN) with three stages: (1) Before problem-solving, we model students' personalized problem space and simulate their acquisition of problem-related knowledge through a gating mechanism. (2) During problem-solving, we evaluate knowledge application and calculate response with a four-parameter IRT. (3) After problem-solving, we quantify student knowledge internalization and forgetting using an incremental indicator. The SNN, inspired by problem-solving and constructivist learning theories, is an interpretable model that attributes learner performance to subjective problems (difficulty, discrimination), objective knowledge (knowledge acquisition and application), and behavior (guessing and slipping). Experimental results show PSKT's advantages in prediction accuracy, reasonable knowledge state assessment, and learning process explanation. The code is available at https://github.com/Oia-10/PSKT.",
            "source": "MM[C]",
            "year": "2024",
            "paper_file_name": "2024-MM-Remembering is Not Applying- Interpretable Knowledge Tracing for Problem-solving Processes.pdf"
        },
        {
            "model_name": "KQN",
            "paper_title": "Knowledge Query Network for Knowledge Tracing: How Knowledge Interacts with Skills",
            "abstract": "Knowledge Tracing (KT) is to trace the knowledge of students as they\nsolve a sequence of problems represented by their related skills. This\ninvolves abstract concepts of students’ states of knowledge and the\ninteractions between those states and skills. Therefore, a KT model\nis designed to predict whether students will give correct answers\nand to describe such abstract concepts. However, existing methods\neither give relatively low prediction accuracy or fail to explain\nthose concepts intuitively. In this paper, we propose a new model\ncalled Knowledge Query Network (KQN) to solve these problems.\nKQN uses neural networks to encode student learning activities\ninto knowledge state and skill vectors, and models the interactions\nbetween the two types of vectors with the dot product. Through\nthis, we introduce a novel concept called probabilistic skill similarity\nthat relates the pairwise cosine and Euclidean distances between\nskill vectors to the odds ratios of the corresponding skills, which\nmakes KQN interpretable and intuitive.\nOn four public datasets, we have carried out experiments to show\nthe following: 1. KQN outperforms all the existing KT models based\non prediction accuracy. 2. The interaction between the knowledge\nstate and skills can be visualized for interpretation. 3. Based on\nprobabilistic skill similarity, a skill domain can be analyzed with\nclustering using the distances between the skill vectors of KQN.\n4. For different values of the vector space dimensionality, KQN\nconsistently exhibits high prediction accuracy and a strong positive\ncorrelation between the distance matrices of the skill vectors.",
            "source": "LAK[C]",
            "year": "2019",
            "paper_file_name": "2019-LAK-KQN@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Structured Knowledge Tracing\nModels for Student Assessment on\nCoursera",
            "abstract": "Massive Open Online Courses (MOOCs) provide an\neﬀective learning platform with various high-quality\neducational materials accessible to learners from all over\nthe world. However, current MOOCs lack personalized\nlearning guidance and intelligent assessment for\nindividuals. Though a few recent attempts have been made\nto trace students' knowledge states by adapting the\npopular Bayesian Knowledge Tracing (BKT) model, they\nhave largely ignored the rich structures and correlations\namong knowledge components (KCs) within a course.\nThis paper proposes to model both the hierarchical and\nthe temporal properties of the knowledge states in order\nto improve the modeling accuracy. Based on the content\norganization characteristics on the Coursera MOOC\nplatform, we provide a well-deﬁned KC model, and\ndevelop Multi-Grained-BKT and Historical-BKT to\ncapture the above features eﬀectively. Experiments on a\nCoursera course dataset show our approach signiﬁcantly\nimproves over previous vanilla BKT models on predicting\nstudents' quiz performance.",
            "source": "L@S[C]",
            "year": "2016",
            "paper_file_name": "2016-L@S-Structured Knowledge Tracing Models for Student Assessment on Coursera.pdf"
        },
        {
            "model_name": "SFKT",
            "paper_title": "No Length Left Behind: Enhancing Knowledge Tracing for Modeling Sequences of Excessive or Insufficient Lengths",
            "abstract": "Knowledge tracing (KT) aims to predict students' responses to practices based on their historical question-answering behaviors. However, most current KT methods focus on improving overall AUC, leaving ample room for optimization in modeling sequences of excessive or insufficient lengths. As sequences get longer, computational costs will increase exponentially. Therefore, KT methods usually truncate sequences to an acceptable length, which makes it difficult for models on online service systems to capture complete historical practice behaviors of students with too long sequences. Conversely, modeling students with short practice sequences using most KT methods may result in overfitting due to limited observation samples. To address the above limitations, we propose a model called Sequence-Flexible Knowledge Tracing (SFKT). Specifically, to flexibly handle long sequences, SFKT introduces a total-term encoder to effectively model complete historical practice behaviors of students at an affordable computational cost. Additionally, to improve the prediction accuracy of students with short practice sequences, we introduce a contrastive learning task and data augmentation schema to improve the generality of modeling short sequences by constructing more learning objectives. Extensive experimental results show that SFKT achieves significant improvements over multiple benchmarks, demonstrating the value of exploring the modeling of sequences of excessive or insufficient lengths. Our code is available at https://github.com/zmy-9/SFKT.",
            "source": "CIKM[C]",
            "year": "2023",
            "paper_file_name": "2023-CIKM-SFKT@@.pdf"
        },
        {
            "model_name": "ATKT",
            "paper_title": "Enhancing Knowledge Tracing via Adversarial Training",
            "abstract": "We study the problem of knowledge tracing (KT) where the goal\nis to trace the students' knowledge mastery over time so as to\nmake predictions on their future performance. Owing to the good\nrepresentation capacity of deep neural networks (DNNs), recent\nadvances on KT have increasingly concentrated on exploring DNNs\nto improve the performance of KT. However, we empirically reveal\nthat the DNNs based KT models may run the risk of overfitting,\nespecially on small datasets, leading to limited generalization. In\nthis paper, by leveraging the current advances in adversarial train-\ning (AT), we propose an efficient AT based KT method (ATKT)\nto enhance KT model's generalization and thus push the limit of\nKT. Specifically, we first construct adversarial perturbations and\nadd them on the original interaction embeddings as adversarial\nexamples. The original and adversarial examples are further used\nto jointly train the KT model, forcing it is not only to be robust to\nthe adversarial examples, but also to enhance the generalization\nover the original ones. To better implement AT, we then present\nan efficient attentive-LSTM model as KT backbone, where the key\nis a proposed knowledge hidden state attention module that adap-\ntively aggregates information from previous knowledge hidden\nstates while simultaneously highlighting the importance of current\nknowledge hidden state to make a more accurate prediction. Exten-\nsive experiments on four public benchmark datasets demonstrate\nthat our ATKT achieves new state-of-the-art performance. Code is\navailable at: https://github.com/xiaopengguo/ATKT.",
            "source": "MM[C]",
            "year": "2021",
            "paper_file_name": "2021-MM-ATKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Convolutional Knowledge Tracing: Modeling Individualization in Student Learning Process",
            "abstract": "With the development of online education systems, a growing number of research works are focusing on Knowledge Tracing (KT), which aims to assess students' changing knowledge state and help them learn knowledge concepts more efficiently. However, only given student learning interactions, most of existing KT methods neglect the individualization of students, i.e., the prior knowledge and learning rates differ from student to student. To this end, in this paper, we propose a novel Convolutional Knowledge Tracing (CKT) method to model individualization in KT. Specifically, for individualized prior knowledge, we measure it from students' historical learning interactions. For individualized learning rates, we design hierarchical convolutional layers to extract them based on continuous learning interactions of students. Extensive experiments demonstrate that CKT could obtain better knowledge tracing results through modeling individualization in learning process. Moreover, CKT can learn meaningful exercise embeddings automatically.",
            "source": "SIGIR[C]",
            "year": "2020",
            "paper_file_name": "2020-SIGIR-Convolutional Knowledge Tracing_ Modeling Individualization__in Student Learning Process-open.pdf"
        },
        {
            "model_name": "SimpleKT",
            "paper_title": "SIMPLEKT: A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR KNOWLEDGE TRACING",
            "abstract": "Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interactions with intelligent tutoring systems. Recently, many works present lots of special methods for applying deep neural networks to KT from different perspectives like model architecture, adversarial augmentation and etc., which make the overall algorithm and system become more and more complex. Furthermore, due to the lack of standardized evaluation protocol (Liu et al., 2022), there is no widely agreed KT baselines and published experimental comparisons become inconsistent and self-contradictory, i.e., the reported AUC scores of DKT on ASSISTments2009 range from 0.721 to 0.821 (Minn et al., 2018; Yeung & Yeung, 2018). Therefore, in this paper, we provide a strong but simple baseline method to deal with the KT task named SIMPLEKT. Inspired by the Rasch model in psychometrics, we explicitly model question-specific variations to capture the individual differences among questions covering the same set of knowledge components that are a generalization of terms of concepts or skills needed for learners to accomplish steps in a task or a problem. Furthermore, instead of using sophisticated representations to capture student forgetting behaviors, we use the ordinary dot-product attention function to extract the time-aware information embedded in the student learning interactions. Extensive experiments show that such a simple baseline is able to always rank top 3 in terms of AUC scores and achieve 57 wins, 3 ties and 16 loss against 12 DLKT baseline methods on 7 public datasets of different domains. We believe this work serves as a strong baseline for future KT research. Code is available at https://github.com/pykt-team/pykt-toolkit1.",
            "source": "ICLR[C]",
            "year": "2023",
            "paper_file_name": "2023-ICLR-SimpleKT@@.pdf"
        },
        {
            "model_name": "DHKT",
            "paper_title": "Deep Hierarchical Knowledge Tracing",
            "abstract": "Knowledge tracing is an essential and challenging task in\nintelligent tutoring systems, whose goal is to estimate stu-\ndents' knowledge state based on their responses to questions.\nAlthough many models for knowledge tracing task are de-\nveloped, most of them depend on either concepts or items as\ninput and ignore the hierarchical structure of items, which\nprovides valuable information for the prediction of student\nlearning results. In this paper, we propose a novel deep hier-\narchical knowledge tracing (DHKT) model exploiting the hi-\nerarchical structure of items. In the proposed DHKT model,\nthe hierarchical relations between concepts and items are\nmodeled by the hinge loss on the inner product between the\nlearned concept embeddings and item embeddings.\nThen\nthe learned embeddings are fed into a neural network to\nmodel the learning process of students, which is used to\nmake predictions. The prediction loss and the hinge loss are\nminimized simultaneously during training process.",
            "source": "EDM[C]",
            "year": "2019",
            "paper_file_name": "2019-EDM-DHKT@@.pdf"
        },
        {
            "model_name": "IKT",
            "paper_title": "Interpretable Knowledge Tracing: Simple and Efﬁcient Student Modeling with Causal Relations",
            "abstract": "Intelligent Tutoring Systems have become critically important in future learning environments. Knowledge Tracing (KT) is a crucial part of that system. It is about inferring the skill mastery of students and predicting their performance to adjust the curriculum accordingly. Deep Learning-based KT models have shown significant predictive performance compared with traditional models. However, it is difficult to extract psychologically meaningful explanations from the tens of thousands of parameters in neural networks, that would relate to cognitive theory. There are several ways to achieve high accuracy in student performance prediction but diagnostic and prognostic reasoning are more critical in learning sciences. Since KT problem has few observable features (problem ID and student's correctness at each practice), we extract meaningful latent features from students' response data by using machine learning and data mining techniques. In this work, we present Interpretable Knowledge Tracing (IKT), a simple model that relies on three meaningful latent features: individual skill mastery, ability profile (learning transfer across skills) and problem difficulty. IKT's prediction of future student performance is made using a Tree-Augmented Naive Bayes Classifier (TAN), therefore its predictions are easier to explain than deep learning-based student models. IKT also shows better student performance prediction than deep learning-based student models without requiring a huge amount of parameters. We conduct ablation studies on each feature to examine their contribution to student performance prediction. Thus, IKT has great potential for providing adaptive and personalized instructions with causal reasoning in real-world educational systems.",
            "source": "AAAI[C]",
            "year": "2022",
            "paper_file_name": "2022-AAAI-IKT@@.pdf"
        },
        {
            "model_name": "MAN",
            "paper_title": "MAN: Memory-augmented Attentive Networks for Deep Learning-based Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) is the task of modeling a learner's knowledge state to predict future performance in e-learning systems based on past performance. Deep learning-based methods, such as recurrent neural networks, memory-augmented neural networks, and attention-based neural networks, have recently been used in KT. Such methods have demonstrated excellent performance in capturing the latent dependencies of a learner's knowledge state on recent exercises. However, these methods have limitations when it comes to dealing with the so-called Skill Switching Phenomenon (SSP), i.e., when learners respond to exercises in an e-learning system, the latent skills in the exercises typically switch irregularly. SSP will deteriorate the performance of deep learning-based approaches for simulating the learner's knowledge state during skill switching, particularly when the association between the switching skills and the previously learned skills is weak. To address this problem, we propose the Memory-augmented Attentive Network (MAN), which combines the advantages of memory-augmented neural networks and attention-based neural networks. Specifically, in MAN, memory-augmented neural networks are used to model learners' longer term memory knowledge, while attention-based neural networks are used to model learners' recent term knowledge. In addition, we design a context-aware attention mechanism that automatically weighs the tradeoff between these two types of knowledge. With extensive experiments on several e-learning datasets, we show that MAN effectively improve predictive accuracies of existing state-of-the-art DLKT methods.",
            "source": "TOIS[T]",
            "year": "2023",
            "paper_file_name": "2023-TOIS-MAN@@Memory-augmented Attentive Networks for Deep Learning-based Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Recent Advances on Deep Learning based Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) is the task of using students' historical\nlearning interaction data to model their knowledge mastery over\ntime so as to make predictions on their future interaction perfor-\nmance. Recently, remarkable progress has been made of using vari-\nous deep learning techniques to solve the KT problem. However, the\nsuccess behind deep learning based knowledge tracing (DLKT) ap-\nproaches is still left somewhat unknown and proper measurement\nand analysis of these DLKT approaches remain a challenge.\nIn this talk, we will comprehensively review recent develop-\nments of applying state-of-the-art deep learning approaches in KT\nproblems, with a focus on those real-world educational data. Be-\nyond introducing the recent advances of various DLKT models, we\nwill discuss how to guarantee valid comparisons across DLKT meth-\nods via thorough evaluations on several publicly available datasets.\nMore specifically, we will talk about (1) KT related psychometric\ntheories; (2) the general DLKT modeling framework that covers\nrecently developed DLKT approaches from different categories;\n(3) the general DLKT benchmark that allows existing approaches\ncomparable on public KT datasets; (4) the broad application of al-\ngorithmic assessment and personalized feedback. Participants will\nlearn about recent trends and emerging challenges in this topic,\nrepresentative tools and learning resources to obtain ready-to-use\nmodels, and how related models and techniques benefit real-world\nKT applications.",
            "source": "WSDM[C]",
            "year": "2023",
            "paper_file_name": "2023-WSDM-Recent Advances on Deep Learning based Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Debiased Cognition Representation Learning for Knowledge Tracing",
            "abstract": "Knowledge tracing (KT) is a fundamental task in intelligent education aimed at tracking students' knowledge status and predicting their performance on new questions. The primary challenge in KT is accurately inferring a high-quality representation of students' knowledge state that effectively captures their understanding of questions. However, existing methods are typically developed under the assumption that students' behaviors directly reflect their knowledge state, which may not hold true especially in online learning scenarios. Abnormal behaviors exhibited by students, such as guessing and plagiarism, can introduce biases into the data, making it difficult to accurately assess students' true knowledge state. To address this limitation, we propose a novel DebiAsed Cognition rEpresentation (DACE) modeling approach. This approach introduces a novel adversarial training strategy based on information bottleneck theory to obtain a debiased knowledge state representation that retains only the most reliable information for accurately predicting students' performance on new questions. Moreover, we design a novel contrastive learning module through embedding-based augmentation to further enhance the robustness and generalizability of the learned knowledge state representation. We conduct extensive experiments on three public KT datasets and the newly released dataset BaiPy to demonstrate the superiority of our model over strong baselines, particularly when confronted with biased data. Our code and datasets are available at https://github.com/lvXiangwei/DACE.git.",
            "source": "TOIS[T]",
            "year": "2025",
            "paper_file_name": "2025-TOIS-Debiased Cognition Representation Learning for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Attentive Model for Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) is a crucial task in the field of on-line education, since it aims to predict students' performance on exercises based on their learning history. One typical solution for knowledge tracing is to combine the classic models in educational psychology, such as Item Response Theory (IRT) and Cognitive Diagnosis (CD), with Deep Neural Networks (DNN) technologies. In this solution, a student and related exercises are mapped into feature vectors based on the student's performance at the current time step, however, it does not consider the impact of historical behavior sequences, and the relationships between historical sequences and students. In this paper, we develop DAKTN, a novel model which assimilates the historical sequences to tackle this challenge for better knowledge tracing. To be specific, we apply a pooling layer to incorporate the student behavior sequence in the embedding layer. After that, we further design a local activation unit, which can adaptively calculate the representation vectors by taking the relevance of historical sequences into consideration with respect to candidate student and exercises. Through experimental results on three real-world datasets, DAKTN significantly outperforms state-of-the-art baseline models. We also present the reasonableness of DAKTN by ablation testing.",
            "source": "AAAI[C]",
            "year": "2023",
            "paper_file_name": "2023-AAAI-Deep Attentive Model for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "MUSE: Multi-Scale Temporal Features Evolution for Knowledge Tracing",
            "abstract": "Transformer based knowledge tracing model is an extensively\nstudied problem in the ﬁeld of computer-aided education. By\nintegrating temporal features into the encoder-decoder struc-\nture, transformers can processes the exercise information and\nstudent response information in a natural way. However, cur-\nrent state-of-the-art transformer-based variants still share two\nlimitations. First, extremely long temporal features cannot\nwell handled as the complexity of self-attention mechanism\nis O(n2). Second, existing approaches track the knowledge\ndrifts under ﬁxed a window size, without considering differ-\nent temporal-ranges. To conquer these problems, we propose\nMUSE, which is equipped with multi-scale temporal sen-\nsor unit, that takes either local or global temporal features\ninto consideration. The proposed model is capable to capture\nthe dynamic changes in users' knowledge states at different\ntemporal-ranges, and provides an efﬁcient and powerful way\nto combine local and global features to make predictions. Our\nmethod won the 5-th place over 3,395 teams in the Riiid AIEd\nChallenge 2020.",
            "source": "AAAI[C]",
            "year": "2021",
            "paper_file_name": "2021-AAAI-Multi-Scale Temporal Features Evolution for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Imputing Knowledge Tracing Data with Subject-Based Training via LSTM Variational Autoencoders Frameworks",
            "abstract": "The issue of missing data poses a great challenge on boosting performance and application of deep learning models in the Knowledge Tracing (KT) problem. However, there has been the lack of understanding on the issue in the literature. In this work, to address this challenge, we adopt a subject-based training method to split and impute data by student IDs instead of row number splitting which we call non-subject based training. The benefit of subject-based training can retain the complete sequence for each student and hence achieve efficient training. Further, we leverage two existing deep generative frameworks, namely variational Autoencoders (VAE) and Longitudinal Variational Autoencoders (LVAE) frameworks and build LSTM kernels into them to form LSTM-VAE and LSTM LVAE (noted as VAE and LVAE for simplicity) models to generate quality data. In LVAE, a Gaussian Process (GP) model is trained to disentangle the correlation between the subject (i.e., student) descriptor information (e.g., age, gender) and the latent space. The paper finally compare the model performance between training the original data and training the data imputed with generated data from non-subject based model VAE-NS and subject-based training models (i.e., VAE and LVAE). We demonstrate that the generated data from LSTM-VAE and LSTM-LVAE can boost the original model performance by about 50%. Moreover, the original model just needs 10% more student data to surpass the original performance if the prediction model is small and 50% more data if the prediction model is large with our proposed frameworks.",
            "source": "AAAI[C]",
            "year": "2023",
            "paper_file_name": "2023-AAAI-Imputing Knowledge Tracing Data with Subject-Based Training via LSTM Variational Autoencoders Frameworks.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases",
            "abstract": "Knowledge tracing (KT) is the task of predicting students' future performance based on their historical learning interaction data. With the rapid advancement of attention mechanisms, many attention based KT models are developed. However, existing attention based KT models exhibit performance drops as the number of student interactions increases beyond the number of interactions on which the KT models are trained. We refer to this as the length generalization of KT model. In this paper, we propose stableKT to enhance length generalization that is able to learn from short sequences and maintain high prediction performance when generalizing on long sequences. Furthermore, we design a multi-head aggregation module to capture the complex relationships between questions and the corresponding knowledge components (KCs) by combining dot-product attention and hyperbolic attention. Experimental results on three public educational datasets show that our model exhibits robust capability of length generalization and outperforms all baseline models in terms of AUC. To encourage reproducible research, we make our data and code publicly available at https://pykt.org.",
            "source": "IJCAI[C]",
            "year": "2024",
            "paper_file_name": "2024-IJCAI-Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks",
            "abstract": "Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a student's code submission passes each test case and 2) the student's open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.",
            "source": "LAK[C]",
            "year": "2025",
            "paper_file_name": "2025-LAK-Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks.pdf"
        },
        {
            "model_name": "",
            "paper_title": "NeurIPS Competition Instructions and Guide: Causal Insights for Learning Paths in Education",
            "abstract": "In this competition, participants will address two fundamental causal challenges in machine learning in the context of education using time-series data. The ﬁrst is to identify the causal relationships between diﬀerent constructs, where a construct is deﬁned as the smallest element of learning. The second challenge is to predict the impact of learning one construct on the ability to answer questions on other constructs. Addressing these challenges will enable optimisation of students' knowledge acquisition, which can be deployed in a real edtech solution impacting millions of students. Participants will run these tasks in an idealised environment with synthetic data and a real-world scenario with evaluation data collected from a series of A/B tests.",
            "source": "NeurIPS[C]",
            "year": "2022",
            "paper_file_name": "2022-NeurIPS-Competition-Causal Insights for Learning Paths in Education.pdf"
        },
        {
            "model_name": "DIMKT",
            "paper_title": "Assessing Student's Dynamic Knowledge State by Exploring the Question Difficulty Effect",
            "abstract": "Knowledge Tracing (KT), which aims to assess students' dynamic knowledge states when practicing on various questions, is a fundamental research task for offering intelligent services in online learning systems. Researchers have devoted significant efforts to developing KT models with impressive performance. However, in existing KT methods, the related question difficulty level, which directly affects students' knowledge state in learning, has not been effectively explored and employed. In this paper, we focus on exploring the question difficulty effect on learning to improve student's knowledge state assessment and propose the DIfficulty Matching Knowledge Tracing (DIMKT) model. Specifically, we first explicitly incorporate the difficulty level into the question representation. Then, to establish the relation between students' knowledge state and the question difficulty level during the practice process, we accordingly design an adaptive sequential neural network in three stages: (1) measuring students' subjective feelings of the question difficulty before practice; (2) estimating students' personalized knowledge acquisition while answering questions of different difficulty levels; (3) updating students' knowledge state in varying degrees to match the question difficulty level after practice. Finally, we conduct extensive experiments on real-world datasets, and the results demonstrate that DIMKT outperforms state-of-the-art KT models. Moreover, DIMKT shows superior interpretability by exploring the question difficulty effect when making predictions. Our codes are available at https://github.com/shshen-closer/DIMKT.",
            "source": "SIGIR[C]",
            "year": "2022",
            "paper_file_name": "2022-SIGIR-DIMKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Context-Embedded Knowledge Tracing and Latent Concept Detection in a Reading Game",
            "abstract": "This study investigates the application of knowledge tracing to the domain of reading comprehension, a complex field characterized by rich contextual data and interrelated concepts. We propose adapting the Dynamic Key-Value Memory Networks (DKVMN) model to incorporate sentence embeddings to better capture the semantic richness of reading tasks, naming our new model Context-embedded DKVMN (CDKVMN). The study employs an extant dataset of 405 students that each completed the reading game \"Map Conquest.\" This game was designed to evaluate students' mastery and use of key reading strategies, such as paraphrasing and bridging. Our findings indicate that CDKVMN outperforms Deep Knowledge Tracing and performs similarly or better than DKVMN in predicting students' performance. This research underscores the potential of advanced, context-sensitive knowledge tracing models to track students' mastery of reading strategies, which can be used to provide support and adapt learning activities to the user. Future work will focus on refining the contextual embeddings, expanding the dataset to a variety of reading games, and interpreting the detected latent concepts.",
            "source": "L@S[C]",
            "year": "2024",
            "paper_file_name": "2024-L@S-Context-Embedded Knowledge Tracing and Latent Concept Detection in a Reading Game.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Knowledge Tracing and Engagement with MOOCs",
            "abstract": "MOOCs and online courses have notoriously high attrition [1]. One\nchallenge is that it can be difficult to tell if students fail to complete\nbecause of disinterest or because of course difficulty. Utilizing a\nDeep Knowledge Tracing framework, we account for student en-\ngagement by including course interaction covariates. With these,\nwe find that we can predict a student's next item response with\nover 88% accuracy. Using these predictions, targeted interventions\ncan be offered to students and targeted improvements can be made\nto courses. In particular, this approach would allow for gating of\ncontent until a student has reasonable likelihood of succeeding.",
            "source": "LAK[C]",
            "year": "2019",
            "paper_file_name": "2019-LAK-Deep Knowledge Tracing and Engagement with MOOCs.pdf"
        },
        {
            "model_name": "LBKT",
            "paper_title": "Learning Behavior-oriented Knowledge Tracing",
            "abstract": "Exploring how learners' knowledge states evolve during the learning activities is a critical task in online learning systems, which can facilitate personalized services downstream, such as course recommendation. Most of existing methods have devoted great efforts to analyzing learners' knowledge states according to their responses (i.e., right or wrong) to different questions. However, the significant effect of learners' learning behaviors (e.g., answering speed, the number of attempts) is omitted, which can reflect their knowledge acquisition deeper and ensure the reliability of the response. In this paper, we propose a Learning Behavior-oriented Knowledge Tracing (LBKT) model, with the goal of explicitly exploring the learning behavior effects on learners' knowledge states. Specifically, we first analyze and summarize several dominated learning behaviors including Speed, Attempts and Hints in the learning process. As the characteristics of different learning behaviors vary greatly, we separately estimate their various effects on learners' knowledge acquisition in a quantitative manner. Then, considering that different learning behaviors are closely dependent with each other, we assess the fused effect of multiple learning behaviors by capturing their complex dependent patterns. Finally, we integrate the forgetting factor with learners' knowledge acquisition to comprehensively update their changing knowledge states in learning. Extensive experimental results on several public datasets demonstrate that our model generates better performance prediction for learners against existing methods. Moreover, LBKT shows good interpretability in tracking learners' knowledge state by incorporating the learning behavior effects. Our codes are available at https://github.com/xbh0720/LBKT.",
            "source": "SIGKDD[C]",
            "year": "2023",
            "paper_file_name": "2023-SIGKDD-LBKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "When is Deep Learning the Best Approach to Knowledge Tracing?",
            "abstract": "Intelligent tutoring systems (ITSs) teach skills using learning-by-doing principles and provide learners with individualized feedback and materials adapted to their level of understanding. Given a learner's history of past interactions with an ITS, a learner performance model estimates the current state of a learner's knowledge and predicts her future performance. The advent of increasingly large scale datasets has turned deep learning models for learner performance prediction into competitive alternatives to classical Markov process and logistic regression models. In an extensive empirical comparison on nine real-world datasets, we ask which approach makes the most accurate predictions and in what conditions. Logistic regression – with the right set of features – leads on datasets of moderate size or containing or containing a very large number of interactions per student, whereas Deep Knowledge Tracing leads on datasets of large size or where precise temporal information matters most. Markov process methods, like Bayesian Knowledge Tracing, lag behind other approaches. We follow this analysis with ablation studies to determine what components of leading algorithms explain their performance and a discussion of model calibration (reliability), which is crucial for downstream applications of learner performance prediction models.",
            "source": "EDM[C]",
            "year": "2020",
            "paper_file_name": "2020-EDM-survey-When is Deep Learning the Best Approach to-open.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Graph Memory Networks for Forgetting-Robust Knowledge Tracing",
            "abstract": "Abstract—Tracing a student's knowledge is vital for tailoring the learning experience. Recent knowledge tracing methods tend to respond to these challenges by modelling knowledge state dynamics across learning concepts. However, they still suffer from several inherent challenges including: modelling forgetting behaviors and identifying relationships among latent concepts. To address these challenges, in this paper, we propose a novel knowledge tracing model, namely Deep Graph Memory Network (DGMN). In this model, we incorporate a forget gating mechanism into an attention memory structure in order to capture forgetting behaviors dynamically during the knowledge tracing process. Particularly, this forget gating mechanism is built upon attention forgetting features over latent concepts considering their mutual dependencies. Further, this model has the capability of learning relationships between latent concepts from a dynamic latent concept graph in light of a student's evolving knowledge states. A comprehensive experimental evaluation has been conducted using four well-established benchmark datasets. The results show that DGMN consistently outperforms the state-of-the-art KTmodels over all the datasets. The effectiveness of modelling forgetting behaviors and learning latent concept graphs has also been analyzed in our experiments.",
            "source": "TKDE[T]",
            "year": "2023",
            "paper_file_name": "2023-TKDE-Deep Graph Memory Networks for Forgetting-Robust Knowledge Tracing.pdf"
        },
        {
            "model_name": "SINKT",
            "paper_title": "SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model",
            "abstract": "Knowledge Tracing (KT) aims to determine whether students will respond correctly to the next question, which is a crucial task in intelligent tutoring systems (ITS). In educational KT scenarios, transductive ID-based methods often face severe data sparsity and cold start problems, where interactions between individual students and questions are sparse, and new questions and concepts consistently arrive in the database. In addition, existing KT models only implicitly consider the correlation between concepts and questions, lacking direct modeling of the more complex relationships in the heterogeneous graph of concepts and questions. In this paper, we propose a Structure-aware INductive Knowledge Tracing model with large language model (dubbed SINKT), which, for the first time, introduces large language models (LLMs) and realizes inductive knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural relationships between concepts and constructs a heterogeneous graph for concepts and questions. Secondly, by encoding concepts and questions with LLMs, SINKT incorporates semantic information to aid prediction. Finally, SINKT predicts the student's response to the target question by interacting with the student's knowledge state and the question representation. Experiments on four real-world datasets demonstrate that SINKT achieves state-of-the-art performance among 12 existing transductive KT models. Additionally, we explore the performance of SINKT on the inductive KT task and provide insights into various modules.",
            "source": "CIKM[C]",
            "year": "2024",
            "paper_file_name": "2024-CIKM-SINKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Learning Patterns-Guided Data Generation for Knowledge Tracing",
            "abstract": "Knowledge tracing (KT), which is instrumental in monitoring and forecasting students' knowledge states throughout their learning trajectory in online learning environments, has over the past decade garnered widespread attention due to its pivotal role in facilitating personalized education. Existing KT approaches were mainly invented from the model-centric perspective to overcome the sequence modeling difficulty while not exploiting the potential information of sparsity, thereby limiting their performance. To make full use of the information in the dataset, this paper proposes a data-centric knowledge tracing paradigm, termed LPDG, aiming to generate interaction data between students and exercises by revealing students' Learning Patterns and facilitating the Generation of ideal training Data. Specifically, we propose a learning patterns-guided exercise sequence regenerator, which incorporates Transformer and a tailor-made pattern enhancer, thereby aiding in the extraction of valuable information for generating high-quality training data. Moreover, we devise a learning patterns-guided pseudo-label generator, which leverages the diffusion process to construct pseudo-labels for the regenerated sequences. Afterwards, the fully generated ideal data is incorporated into the training data, and we integrate this framework with various model-centric approaches in KT. Finally, experimental results across datasets clearly demonstrate the efficacy of our proposed LPDG framework.",
            "source": "SIGKDD[C]",
            "year": "2025",
            "paper_file_name": "2025-SIGKDD-Learning Patterns-Guided Data Generation for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Prerequisite-Driven Deep Knowledge Tracing",
            "abstract": "Abstract—Knowledge tracing serves as the key technique in\nthe computer supported education environment (e.g., intelligent\ntutoring systems) to model student’s knowledge states. While the\nBayesian knowledge tracing and deep knowledge tracing models\nhave been developed, the sparseness of student’s exercise data\nstill limits knowledge tracing’s performance and applications.\nIn order to address this issue, we advocate for and propose\nto incorporate the knowledge structure information, especially\nthe prerequisite relations between pedagogical concepts, into\nthe knowledge tracing model. Speciﬁcally, by considering how\nstudents master pedagogical concepts and their prerequisites,\nwe model prerequisite concept pairs as ordering pairs. With a\nproper mathematical formulation, this property can be utilized as\nconstraints in designing knowledge tracing model. As a result, the\nobtained model can have a better performance on student concept\nmastery prediction. In order to evaluate this model, we test it on\nﬁve different real world datasets, and the experimental results\nshow that the proposed model achieves a signiﬁcant performance\nimprovement by comparing with three knowledge tracing models.",
            "source": "ICDM[C]",
            "year": "2018",
            "paper_file_name": "2018-ICDM-Prerequisite-Driven_Deep_Knowledge_Tracing.pdf"
        },
        {
            "model_name": "PSI-KT",
            "paper_title": "PREDICTIVE, SCALABLE AND INTERPRETABLE KNOWLEDGE TRACING ON STRUCTURED DOMAINS",
            "abstract": "Intelligent tutoring systems optimize the selection and timing of learning materials\nto enhance understanding and long-term retention. This requires estimates of both\nthe learner's progress (\"knowledge tracing\"; KT), and the prerequisite structure\nof the learning domain (\"knowledge mapping\"). While recent deep learning\nmodels achieve high KT accuracy, they do so at the expense of the interpretability\nof psychologically-inspired models. In this work, we present a solution to this\ntrade-off. PSI-KT is a hierarchical generative approach that explicitly models\nhow both individual cognitive traits and the prerequisite structure of knowledge\ninfluence learning dynamics, thus achieving interpretability by design. Moreover,\nby using scalable Bayesian inference, PSI-KT targets the real-world need for\nefficient personalization even with a growing body of learners and learning histories.\nEvaluated on three datasets from online learning platforms, PSI-KT achieves\nsuperior multi-step predictive accuracy and scalable inference in continual-learning\nsettings, all while providing interpretable representations of learner-specific traits\nand the prerequisite structure of knowledge that causally supports learning. In\nsum, predictive, scalable and interpretable knowledge tracing with solid knowledge\nmapping lays a key foundation for effective personalized learning to make education\naccessible to a broad, global audience.",
            "source": "ICLR[C]",
            "year": "2024",
            "paper_file_name": "2024-ICLR-PSI-KT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Cold Start Knowledge Tracing with Attentive Neural Turing Machine",
            "abstract": "Deep learning based knowledge tracing approaches achieve high accuracy in mastery prediction with pattern extraction on a large learning behavior data set. However, when there is little training data available, these approaches either fail to extract the key patterns or result in over ﬁtting. Ideally, we aim to provide a similar learning experience to both the ﬁrst group of learners, who interact with a new course or a new activity with little learning behavior data to provide personalized guidance, and the learners who interact with the course later. We propose a novel architecture, Attentive Neural Turing Machine (ANTM), to solve the cold start knowledge tracing problem. The proposed ANTM comprises an attentive controller module and differential reading and writing processes with extra memory bank. Accuracy (ACC) and Area Under Curve (AUC) measures are used for model performance comparison. Results show the proposed approach can learn fast and generalize well to unseen data. It achieves around 95% ACC trained with only 3 learners, while conventional deep learning based approaches achieve only 65% ACC with over prediction issues.",
            "source": "L@S[C]",
            "year": "2020",
            "paper_file_name": "2020-L@S-Cold Start Knowledge Tracing with Attentive Neural Turing Machine.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Discovering Multi-Relational Integration for Knowledge Tracing with Retentive Networks",
            "abstract": "Knowledge Tracing (KT) focuses on estimating students' knowledge states and predicting their future performances, which is a crucial task for online education platforms. In light of the advancements in educational big data and deep neural networks, numerous KT models have been proposed and promising outcomes have been achieved. Nevertheless, we have noted that current methods possess certain evident constraints. Thus, we propose a Knowledge Tracing model with Multi-Relational Integration (MRIKT): (1) we consider the more sophisticated relations between questions and skills, which can reveal deeper patterns of students' learning; (2) we emphasize the forgetfulness nature of students and the value of inter-exercises relations by incorporating a retentive module. Specifically, we choose graph convolutional networks to construct the advanced-relation between questions and skills, named graph representation module. Additionally, by linking different exercises, our novel retentive module, inspired by RetNet, can acquire valuable insights. We extensively evaluate the performance of MRIKT on three real-world datasets. The results demonstrate that MRIKT achieves outstanding performance, which improves at least 8.44% compared to baseline models.",
            "source": "ICMR[C]",
            "year": "2024",
            "paper_file_name": "2024-ICMR-Discovering Multi-Relational Integration for Knowledge Tracing with Retentive Networks.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Question Embedding on Weighted Heterogeneous Information Network for Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) aims to predict students' future performance on answering questions based on their historical exercise sequences. To alleviate the problem of data sparsity in KT, recent works have introduced auxiliary information to mine question similarity, resulting in the enhancement of question embeddings. Nonetheless, there remains a gap in developing an approach that effectively incorporates various forms of auxiliary information, including relational information (e.g., question–student, question–skill relation), relationship attributes (e.g., correctness indicating a student's performance on a question), and node attributes (e.g., student ability). To tackle this challenge, the Similarity-enhanced Question Embedding (SimQE) method for KT is proposed, with its central feature being the utilization of weighted and attributed meta-paths for extracting question similarity. To capture multi-dimensional question similarity semantics by integrating multiple relations, various meta-paths are constructed for learning question embeddings separately. These embeddings, each encoding different similarity semantics, are then fused to serve the task of KT. To capture finer-grained similarity by leveraging the relationship attributes and node attributes on the meta-paths, the biased random walk algorithm is designed. In addition, the auxiliary node generation method is proposed to capture high-order question similarity. Finally, extensive experiments conducted on six datasets demonstrate that SimQE performs the best among 10 representative question embedding methods. Furthermore, SimQE proves to be more effective in alleviating the problem of data sparsity.",
            "source": "TKDD[T]",
            "year": "2024",
            "paper_file_name": "2024-TKDD-Question Embedding on Weighted Heterogeneous Information Network for Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "No Task Left Behind: Multi-Task Learning of Knowledge Tracing and Option Tracing for Better Student Assessment",
            "abstract": "Student assessment is one of the most fundamental tasks in\nthe field of AI Education (AIEd). One of the most common\napproach to student assessment is Knowledge Tracing (KT),\nwhich evaluates a student's knowledge state by predicting\nwhether the student will answer a given question correctly\nor not. However, in the context of multiple choice (polyto-\nmous) questions, conventional KT approaches are limited in\nthat they only consider the binary (dichotomous) correctness\nlabel (i.e., correct or incorrect), and disregard the specific op-\ntion chosen by the student. Meanwhile, Option Tracing (OT)\nattempts to model a student by predicting which option they\nwill choose for a given question, but overlooks the correct-\nness information. In this paper, we propose Dichotomous-\nPolytomous Multi-Task Learning (DP-MTL), a multi-task\nlearning framework that combines KT and OT for more pre-\ncise student assessment. In particular, we show that the KT\nobjective acts as a regularization term for OT in the DP-MTL\nframework, and propose an appropriate architecture for ap-\nplying our method on top of existing deep learning-based\nKT models. We experimentally confirm that DP-MTL sig-\nnificantly improves both KT and OT performances, and also\nbenefits downstream tasks such as Score Prediction (SP).",
            "source": "AAAI[C]",
            "year": "2022",
            "paper_file_name": "2022-AAAI-No Task Left Behind.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction",
            "abstract": "This paper presents novel techniques for enhancing the performance of knowledge tracing (KT) models by focusing on the crucial factor of question and concept difficulty level. Despite the acknowledged significance of difficulty, previous KT research has yet to exploit its potential for model optimization and has struggled to predict difficulty from unseen data. To address these problems, we propose a difficulty-centered contrastive learning method for KT models and a Large Language Model (LLM)-based framework for difficulty prediction. These innovative methods seek to improve the performance of KT models and provide accurate difficulty estimates for unseen data. Our ablation study demonstrates the efficacy of these techniques by demonstrating enhanced KT model performance. Nonetheless, the complex relationship between language and difficulty merits further investigation.",
            "source": "ACL[C]",
            "year": "2024",
            "paper_file_name": "2024-ACL-Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Efficient Transformer-based Knowledge Tracing for a Personalized Language Education Application",
            "abstract": "The purpose of this paper is to propose a new deep learning-based approach to recommend highly personalized educational contents to learners. Towards this goal, we present a knowledge tracing algorithm by adding long short-term memory units to a Transformer-based model. By inferring the knowledge state of a learner through the proposed KT algorithm, it not only removes problems that the learner does not have to solve but also suggest problems so that the learner's knowledge state level improves most efficiently. In this manner, a personalized educational curriculum can be provided to each learner. We trained the model with 90 million datasets collected from a Hangeul (i.e., Korean character) learning mobile application called \"Sojung-Hangeul\", one of the market-leading Korean learning services. The experimental results show that the AUC of the proposed model significantly improves from 0.88 to 0.92 compared to the recent Transformer-based approach in real-time environments. The proposed deep learning model is applied into \"Sojung-Hangeul\", and the application is currently available at https://bit.ly/Sojung-Hangeul.",
            "source": "L@S[C]",
            "year": "2023",
            "paper_file_name": "2023-L@S-Efficient Transformer-based Knowledge Tracing for a Personalized Language Education Application.pdf"
        },
        {
            "model_name": "DKT+",
            "paper_title": "Addressing Two Problems in Deep Knowledge Tracing via Prediction-Consistent Regularization",
            "abstract": "Knowledge tracing is one of the key research areas for empowering personalized education. It is a task to model students' mastery level of a knowledge component (KC) based on their historical learning trajectories. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, through our extensive experimentation, we have noticed two major problems in the DKT model. The first problem is that the model fails to reconstruct the observed input. As a result, even when a student performs well on a KC, the prediction of that KC's mastery level decreases instead, and vice versa. Second, the predicted performance for KCs across time-steps is not consistent. This is undesirable and unreasonable because student's performance is expected to transit gradually over time. To address these problems, we introduce regularization terms that correspond to reconstruction and waviness to the loss function of the original DKT model to enhance the consistency in prediction. Experiments show that the regularized loss function effectively alleviates the two problems without degrading the original task of DKT.",
            "source": "L@S[C]",
            "year": "2018",
            "paper_file_name": "2018-L@S-DKT+@@.pdf"
        },
        {
            "model_name": "AdaptKT",
            "paper_title": "AdaptKT: A Domain Adaptable Method for Knowledge Tracing",
            "abstract": "Knowledge tracing is a crucial and fundamental task in online education systems, which can predict students' knowledge state for personalized learning. Unfortunately, existing methods are domain-specific, whereas there are many domains (e.g., subjects, schools) in the real education scene and some domains suffer from the problem of lacking sufficient data. Therefore, exploiting the knowledge among other domains to improve the model's performance in the target domain remains pretty much open. We term this problem as Domain Adaptation for Knowledge Tracing (DAKT), which aims to transfer knowledge from the source domain to the target one for knowledge tracing. In this paper, we propose a novel adaptable method, namely Adaptable Knowledge Tracing (AdaptKT), which contains three phases to explore this problem. Specifically, phase I is instance selection. Given the question texts of two domains, we train an auto-encoder to select and embed similar instances from both domains. Phase II is distribution discrepancy minimizing. After obtaining the selected instances and their linguistic representations, we train a knowledge tracing model and adopt the Maximum Mean Discrepancy (MMD) to minimize the discrepancy between the distributions of the domain-specific knowledge states. Phase III is fine-tuning of the output layer. We replace the output layer of the model that trained in phase II with a new one to make the knowledge tracing model's output dimension match the number of knowledge concepts in the target domain. The new output layer is trained while other parameters are frozen. We conduct extensive experiments on two large-scale real-world datasets, where the experimental results clearly demonstrate the effectiveness of AdaptKT for solving the DAKT problem.",
            "source": "WSDM[C]",
            "year": "2022",
            "paper_file_name": "2022-WSDM-AdaptKT@@.pdf"
        },
        {
            "model_name": "",
            "paper_title": "DialogID: A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments",
            "abstract": "Online dialogic instructions are a set of pedagogical instructions\nused in real-world online educational contexts to motivate students,\nhelp understand learning materials, and build effective study habits.\nIn spite of the popularity and advantages of online learning, the ed-\nucation technology and educational data mining communities still\nsuffer from the lack of large-scale, high-quality, and well-annotated\nteaching instruction datasets to study computational approaches\nto automatically detect online dialogic instructions and further\nimprove the online teaching effectiveness. Therefore, in this pa-\nper, we present a dataset of online dialogic instruction detection,\nDialogID, which contains 30,431 effective dialogic instructions.\nThese teaching instructions are well annotated into 8 categories.\nFurthermore, we utilize the prevalent pre-trained language mod-\nels (PLMs) and propose a simple yet effective adversarial training\nlearning paradigm to improve the quality and generalization of\ndialogic instruction detection. Extensive experiments demonstrate\nthat our approach outperforms a wide range of baseline methods.\nThe data and our code are available for research purposes from:\nhttps://github.com/ai4ed/DialogID.",
            "source": "CIKM[C]",
            "year": "2022",
            "paper_file_name": "2022-CIKM-DialogID- A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Question Difficulty Consistent Knowledge Tracing",
            "abstract": "Knowledge tracing aims to estimate knowledge states of students over a set of skills based on students' past learning activities. Deep learning based knowledge tracing models show superior performance to traditional knowledge tracing approaches. Early works like DKT use skill IDs and student responses only. Recent works also incorporate questions IDs into their models and achieve much improved performance in the next question correctness prediction task. However, predictions made by these models are thus on specific questions, and it is not straightforward to translate them to estimation of students' knowledge states over skills. In this paper, we propose to replace question IDs with question difficulty levels in deep knowledge tracing models. The predictions made by our model can be more readily translated to students' knowledge states over skills. Furthermore, by using question difficulty levels to replace question IDs, we can also alleviate the cold-start problem in knowledge tracing as online learning platforms are updated frequently with new questions. We further use two techniques to smooth the predicted scores. One is to combine embeddings of nearby difficulty levels using the Hann function. The other is to constrain the predicted probabilities to be consistent with question difficulties by imposing a penalty if they are not consistent. We conduct extensive experiments to study the performance of the proposed model. Our experimental results show that our model outperforms the state-of-the-art knowledge tracing models in terms of both accuracy and consistency with question difficulty levels.",
            "source": "WWW[C]",
            "year": "2024",
            "paper_file_name": "2024-WWW-Question Difficulty Consistent Knowledge Tracing.pdf"
        },
        {
            "model_name": "",
            "paper_title": "Deep Knowledge Tracing with Learning Curves",
            "abstract": "Abstract—Knowledge tracing (KT) models students’ mastery\nlevel of knowledge concepts based on their responses to the\nquestions in the past and predicts the probability that they\ncorrectly answer subsequent questions in the future. Recent KT\nmodels are mostly developed with deep neural networks and have\ndemonstrated superior performance over traditional approaches.\nHowever, they ignore the explicit modeling of the learning curve\ntheory, which generally says that more practices on the same\nknowledge concept enhance one’s mastery level of the concept.\nBased on this theory, we propose a Convolution-Augmented\nKnowledge Tracing (CAKT) model to enable learning curve\nmodeling. In particular, when predicting a student’s response to\nthe next question associated with a speciﬁc knowledge concept,\nCAKT uses a module built with three-dimensional convolutional\nneural networks to learn the student’s recent experience on that\nconcept. Moreover, CAKT employs LSTM networks to learn the\noverall knowledge state, which is fused with the feature learned\nby the convolutional module. As such, CAKT can learn the\nstudent’s overall knowledge state as well as the knowledge state\nof the concept in the next question. Experimental results on four\nreal-life datasets show that CAKT achieves better performance\ncompared to existing deep KT models.",
            "source": "ICDM[C]",
            "year": "2022",
            "paper_file_name": "2022-ICDM-Deep Knowledge Tracing with Learning Curves.pdf"
        }
    ]
}